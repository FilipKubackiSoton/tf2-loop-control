{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow2 training loop control using default *tf.fit(...)* function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Descripton\n",
    "\n",
    "Up to now custom training loop in Tensorflow2 requires writing two lops:\n",
    "1. loop iterating through epochs \n",
    "2. loop iterating through batches \n",
    "\n",
    "Then all castom training precudere will have to be implemented in these double-loop block of code. It's neither elegant nor robust due to the missing advanced features of *tf.fit(...)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import functools\n",
    "from tensorflow_addons.utils import types\n",
    "from typeguard import typechecked\n",
    "import numpy as np\n",
    "import argparse\n",
    "from scipy.stats import truncnorm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t 10\n",
      "\t\tmean(s)\t 12.5\n",
      "\t\tdata <\t 15\n",
      "\t\tstd \t 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-o\", \"--output\", dest=\"output\", default=\"naly_syn_simple_arith\")\n",
    "parser.add_argument(\"-d\", \"--dist\", dest=\"dist\", default=\"normal\", help=\"Prob.Dist\")\n",
    "parser.add_argument(\n",
    "    \"-p\", \"--params\", dest=\"params\", default=\"(-3,3)\", type=ast.literal_eval\n",
    ")\n",
    "parser.add_argument(\"-e\", \"--ext\", dest=\"ext\", default=\"(10,15)\", type=ast.literal_eval)\n",
    "\n",
    "parser.add_argument(\"-n\", \"--nalu\", dest=\"nalu\", default=\"nalui1\")\n",
    "parser.add_argument(\"-se\", \"--seed\", dest=\"seed\", default=42, type=int)\n",
    "parser.add_argument(\"-op\", \"--operation\", dest=\"op\", default=\"MUL\")\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "def sample(dist, params, numDim=3, numDP=64000):\n",
    "    data = np.zeros(shape=(numDP, numDim))\n",
    "    if dist == \"normal\":\n",
    "        intmean = (params[0] + params[1]) / 2\n",
    "        intstd = (params[1] - params[0]) / 6\n",
    "        print(\n",
    "            \"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tmean(s)\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\tstd \\t %s\"\n",
    "            % (dist, params[0], intmean, params[1], intstd)\n",
    "        )\n",
    "        mi, ma = (params[0] - intmean) / intstd, (params[1] - intmean) / intstd\n",
    "        data = np.reshape(\n",
    "            truncnorm.rvs(mi, ma, intmean, intstd, size=numDim * numDP), data.shape\n",
    "        )\n",
    "\n",
    "    elif dist == \"uniform\":\n",
    "        print(\n",
    "            \"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\t\"\n",
    "            % (dist, params[0], params[1])\n",
    "        )\n",
    "        data = np.reshape(\n",
    "            np.random.uniform(params[0], params[1], size=numDim * numDP), data.shape\n",
    "        )\n",
    "    elif dist == \"exponential\":\n",
    "        data = np.random.exponential(params, size=(numDP, numDim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown distribution\")\n",
    "    data = np.reshape(data, [-1])  # reshape to mix both distributions per instance!\n",
    "    np.random.shuffle(data)\n",
    "    data = np.reshape(data, (numDP, numDim))\n",
    "    return data\n",
    "\n",
    "def operation(op, a, b):\n",
    "    if op.lower() == \"mul\":\n",
    "        return a * b\n",
    "    if op.lower() == \"add\":\n",
    "        return a + b\n",
    "    if op.lower() == \"sub\":\n",
    "        return a - b\n",
    "    if op.lower() == \"div\":\n",
    "        return a / b\n",
    "\n",
    "input_dim = 7\n",
    "output_dim = 1\n",
    "\n",
    "def data_comb(data):\n",
    "    return (data[:, 0] - data[:, 1]) *  (data[:, 2] - data[:, 3]) +  (data[:, 4] * data[:, 5]) \n",
    "\n",
    "data = sample(args.dist, args.params, input_dim)\n",
    "lbls =  data_comb(data)\n",
    "lbls = np.reshape(lbls, newshape=(-1, 1))\n",
    "\n",
    "int_data = sample(args.dist, args.params, input_dim)\n",
    "int_lbls =  data_comb(int_data)\n",
    "int_lbls = np.reshape(int_lbls, newshape=(-1, 1))\n",
    "\n",
    "\n",
    "ext_data = sample(args.dist, args.ext, input_dim)\n",
    "ext_lbls =  data_comb(ext_data)\n",
    "ext_lbls = np.reshape(ext_lbls, newshape=(-1, 1))\n",
    "\n",
    "\n",
    "data_dp = tf.data.Dataset.from_tensor_slices((data, lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "int_data_dp = tf.data.Dataset.from_tensor_slices((int_data, int_lbls)).prefetch(tf.data.AUTOTUNE)\n",
    "ext_data_dp = tf.data.Dataset.from_tensor_slices((ext_data, ext_lbls)).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind(instance, func, as_name=None):\n",
    "    \"\"\"\n",
    "    Bind the function *func* to *instance*, with either provided name *as_name*\n",
    "    or the existing name of *func*. The provided *func* should accept the \n",
    "    instance as the first argument, i.e. \"self\".\n",
    "    \"\"\"\n",
    "    if as_name is None:\n",
    "        as_name = func.__name__\n",
    "    bound_method = func.__get__(instance, instance.__class__)\n",
    "    setattr(instance, as_name, bound_method)\n",
    "    return bound_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopControlerCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, \n",
    "        config: int, \n",
    "        reinit_config: Dict[str, Any] = None, \n",
    "        default_in_branch: Dict[str, Any] = {\n",
    "                \"loss\": True, \n",
    "                \"regularize\": None, \n",
    "                \"clipping\": None,\n",
    "                \"variables\": None,\n",
    "                \"excluded_variables\": None,\n",
    "            },\n",
    "            verbose: bool = True,\n",
    "            *args, **kwargs\n",
    "    ) -> None:\n",
    "        super(LoopControlerCallback, self).__init__(*args, **kwargs)\n",
    "        self.default_in_branch: Dict[str, Any] = default_in_branch\n",
    "        self.config: Dict[str, Any] = config\n",
    "        self.reinit_config = reinit_config\n",
    "        self.verbose: bool = verbose\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"Function called directely before training. It is executed on each call of model.fit with this callback.\n",
    "            Inside the scope of this funciton we can access model on which this callback works: self.model.\n",
    "\n",
    "        Args:\n",
    "            logs (_type_, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        # meta attributes for building conditions\n",
    "        self.epochs: int = 0\n",
    "        self.batches: int = 0\n",
    "        self.history: List[Any] = []\n",
    "\n",
    "        # meta attributes for reinitialization handeling\n",
    "        self.reinit_epochs: int = 0\n",
    "        self.reinit_batches: int = 0\n",
    "        self.reinit_history: List[Any] = []\n",
    "        self.reinit_current_history: List[Any] = []\n",
    "        \n",
    "        # extend config with validation step\n",
    "        self.config = self._extend_config(self.config)\n",
    "\n",
    "        # assign active and exluded variables for slave steps\n",
    "        self._extract_substeps_varialbe_arrays(self.config)\n",
    "\n",
    "        # bind control variables and control conditions\n",
    "        self._bind_controlers(self.config)\n",
    "        \n",
    "        # bind master train_step to model\n",
    "        self._bind_master_step(self.config)\n",
    "\n",
    "        # bind slave train_steps to model\n",
    "        self._bind_slaves_steps(self.config)\n",
    "\n",
    "        # bind reinitalization functionality to model only if reinitalizatoin config is provided\n",
    "        if self.reinit_config:\n",
    "            self._bind_reinitialization(self.reinit_config)\n",
    "\n",
    "\n",
    "        # test\n",
    "        self.model.optimizer.build(self.model.trainable_variables) \n",
    "\n",
    "    def _bind_master_step(self, config: Dict[str, Any]) -> None:\n",
    "        lscope = locals()\n",
    "        def _get_losses(config: Dict[str, Any]) -> str:\n",
    "            def _substeps_condition_execution(name: str, config: Dict[str, Any], on: bool) -> str:\n",
    "                if on:\n",
    "                    return f\"self.{name}_on(data)\" if True in config else \"0.0\"\n",
    "                else:\n",
    "                    return f\"self.{name}_off(data)\" if False in config else \"0.0\"\n",
    "                    \n",
    "            return \"{\" + \",\".join([\n",
    "                        f\"'loss_{an}' : tf.cond(self.control_variables['{an}'], lambda: {_substeps_condition_execution(an, ac, True)}, lambda: {_substeps_condition_execution(an, ac, False)})\"\n",
    "                        for an, ac in config.items()\n",
    "                    ]) + \"}\"\n",
    "\n",
    "        lscope = locals()\n",
    "        function_body = \"\"\"\n",
    "@tf.function\n",
    "def train_step(self, data):\n",
    "    loss = {losses_config}\n",
    "    metrics = {{m.name : m.result() for m in self.metrics}}\n",
    "    # if \"loss\" not in metrics and self.loss in self.metrics_names:\n",
    "    #     metrics[\"loss\"] = metrics.pop(self.loss)\n",
    "    control_states = {{\n",
    "            control_name: tf.cond(\n",
    "                control_value,\n",
    "                lambda: tf.constant(True),\n",
    "                lambda: tf.constant(False),\n",
    "            )\n",
    "            for control_name, control_value in self.control_variables.items()\n",
    "    }}\n",
    "    \n",
    "    return {{**loss, **metrics, **control_states}}\n",
    "\"\"\".format(\n",
    "            **{\"losses_config\": _get_losses(config)}\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\n-------------------MASTER STEP-------------------\")\n",
    "            print(function_body)\n",
    "        \n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[\"train_step\"])\n",
    "\n",
    "    def _bind_slaves_steps(self, config) -> None:\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"-------------------SLAVE STEPS-------------------\\n\")\n",
    "\n",
    "        for action_name, action_config in config.items():\n",
    "            if True in action_config:\n",
    "                self._bind_slave_step(action_name, action_config[True], True)\n",
    "            if False in action_config:\n",
    "                self._bind_slave_step(action_name, action_config[False], False)\n",
    "    \n",
    "    def _bind_slave_step(self, action_name: str, fn_config: Dict[str, Any], branch: bool) -> None:\n",
    "        \n",
    "\n",
    "        lscope = {\n",
    "            **locals(), \n",
    "            **fn_config\n",
    "            }\n",
    "        fn_name = self._get_actoin_step_name(action_name, branch)\n",
    "        if fn_config[\"loss\"]==False:\n",
    "            # dummy error that will be anyway scale by 0 to make graph to compile otherwise\n",
    "            # ---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
    "            # ValueError: None values not supported.\n",
    "            #######################################\n",
    "            # OBSERVATION: loss function with y_pred and y_true must be consumed inside the gradient tape scope\n",
    "            #######################################\n",
    "            lscope[\"loss\"] = self.model.compiled_loss\n",
    "\n",
    "        function_body = f\"\"\"\n",
    "@tf.function\n",
    "def {fn_name}(self, data):\n",
    "    x, y = data\n",
    "    with tf.GradientTape(watch_accessed_variables={'False' if fn_config[\"variables\"] or fn_config[\"excluded_variables\"] else 'True'}) as tape:\n",
    "        \"\"\"\n",
    "\n",
    "        if fn_config[\"variables\"] or fn_config[\"excluded_variables\"]:\n",
    "            function_body += f\"\"\"\n",
    "        for g in self.{'_included_variables' if fn_config[\"variables\"] else '_excluded_variables' }['{action_name}'][{branch}]:\n",
    "            tape.watch(g)\n",
    "            \"\"\"\n",
    "        \n",
    "        function_body += f\"\"\"\n",
    "        logits = self(x, training=True)\n",
    "        loss_value = {'tf.constant(0, dtype=tf.float32) *'  if fn_config[\"loss\"]==False else ''} loss(y, logits)\n",
    "        \"\"\"\n",
    "        # loss_value = {'loss(y, logits)' if fn_config[\"loss\"] else 'tf.constant(0, dtype=tf.float32)'}\n",
    "        # loss_value = {'loss(y, logits)' if fn_config[\"loss\"] else 'tf.math.reduce_sum(self.losses)'}\n",
    "\n",
    "        if fn_config[\"regularize\"]:\n",
    "            function_body += f\"\"\"\n",
    "        loss_value += tf.math.reduce_sum(self.losses)\n",
    "        \"\"\"\n",
    "    \n",
    "        function_body += f\"\"\"\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip({{clipping_grads}}, tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "    return loss_value\n",
    "\"\"\".format(**{\n",
    "    \"clipping_grads\": \"[tf.clip_by_value(g, {clip_low}, {clip_high}) for g in grads]\".format(**{\n",
    "        \"clip_low\": fn_config[\"clipping\"][0], \"clip_high\": fn_config[\"clipping\"][1]\n",
    "        }) if fn_config[\"clipping\"] else \"grads\",\n",
    "    \"regularize_loss_add\": \"loss_value += sum(self.losses)\" if fn_config[\"regularize\"]==True else \"\"\n",
    "})\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"-------------------{fn_name}-------------------\")\n",
    "            print(function_body)\n",
    "        \n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[fn_name])\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs) -> None:\n",
    "        self.epochs += 1\n",
    "        self.reinit_epochs += 1\n",
    "        \"\"\"Control gating variable from the level of callback which can work on epoch/batch level.\"\"\"\n",
    "        # tf.variable.assign is different than tf.variable = <sth>. The second option is compiled to static\n",
    "        # value in TF graph of computation as the result of @tf.function decorators in LoopControlableModel\n",
    "        for action_name, _ in self.config.items():\n",
    "            self.model.control_variables[action_name].assign(\n",
    "                getattr(self, self.control_conditions[action_name])()\n",
    "            )\n",
    "        \n",
    "        # reinitlaization \n",
    "        if self.reinit_config and self.reinitialize_cond():\n",
    "            self.reinitialize()\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.batches += 1\n",
    "        self.reinit_batches +=1\n",
    "        \"\"\"Control gating variable from the level of callback which can work on epoch/batch level.\"\"\"\n",
    "        # tf.variable.assign is different than tf.variable = <sth>. The second option is compiled to static\n",
    "        # value in TF graph of computation as the result of @tf.function decorators in LoopControlableModel\n",
    "        for action_name, _ in self.config.items():\n",
    "            self.model.control_variables[action_name].assign(\n",
    "                getattr(self, self.control_conditions[action_name])()\n",
    "            )\n",
    "        \n",
    "        self.history.append(logs.get(\"loss\"))\n",
    "        self.reinit_history.append(logs.get(\"loss\"))\n",
    "        \n",
    "    def _get_actoin_step_name(self, action_name: str, branch: bool) -> str:\n",
    "        return f\"{action_name}_on\" if branch else f\"{action_name}_off\"\n",
    "\n",
    "    def _bind_controlers(self, config) -> None:\n",
    "        self.model.control_variables = {}\n",
    "        self.control_conditions = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            self.model.control_variables[action_name] = tf.Variable(False, trainable=False)\n",
    "            condition_function_name = action_name + \"_condition\"\n",
    "            bind(self, action_config[\"cond\"], condition_function_name)\n",
    "            self.control_conditions[action_name] = condition_function_name\n",
    "\n",
    "    def _extend_config(self, config: Dict[str, Any]) -> None:\n",
    "        \"\"\"Extend and validate config file. Fill missing fields based on the default_in_branch.\n",
    "\n",
    "        Args:\n",
    "            config (Dict[str, Any]): Configuration to control model training\n",
    "        \"\"\"\n",
    "\n",
    "        def validate_action_config(action_name: str, action_config: Dict[str, Any]) -> None:\n",
    "            \"\"\"Validate model training configuration.\n",
    "\n",
    "            Args:\n",
    "                action_name (str): name of the action slave train step\n",
    "                action_config (Dict[str, Any]): configuration of the action slave train step\n",
    "\n",
    "            Raises:\n",
    "                ValueError: Missing controlable cond\n",
    "                ValueError: Missing branch configuration for true/false after cond\n",
    "            \"\"\"\n",
    "\n",
    "            Warning(f\"------Validating Configuration for {action_name}------\")\n",
    "            if action_config == {}:\n",
    "                Warning(\n",
    "                    f\"{action_name} has empty body. Condition and False or True branch must be implemented.\\n It's ignored in furhter computations\"\n",
    "                )\n",
    "            if (True not in action_config) and (False not in action_config):\n",
    "                raise ValueError(\n",
    "                    f\"{action_name} has no False or True branch implemented\"\n",
    "                )\n",
    "            if \"cond\" not in action_config:\n",
    "                raise ValueError(f\"{action_name} has no condition implemented.\")\n",
    "            \n",
    "            # if \"loss\" in action_config and action_config[\"loss\"]==None and \"regularize\" not in action_config:\n",
    "            #     raise ValueError(f\"{action_name} has both loss None and regularize None what makes loss error const zero.\")\n",
    "\n",
    "\n",
    "        class DummyZeroLoss(tf.keras.losses.Loss):\n",
    "            def call(self, y_true, _):\n",
    "                return tf.reduce_mean(y_true - y_true, axis=-1)\n",
    "            \n",
    "        # if loss in default branch is None, then use compiled loss\n",
    "        if self.default_in_branch[\"loss\"]==True:\n",
    "            self.default_in_branch[\"loss\"] = self.model.compiled_loss\n",
    "\n",
    "        pc = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            validate_action_config(action_name, action_config)\n",
    "            pc[action_name] = {\"cond\": action_config[\"cond\"]}\n",
    "            if True in action_config:\n",
    "                pc[action_name][True] = {**self.default_in_branch, **action_config[True]}\n",
    "                if pc[action_name][True][\"loss\"]==True:\n",
    "                    pc[action_name][True][\"loss\"]=self.model.compiled_loss\n",
    "\n",
    "            if False in action_config:\n",
    "                pc[action_name][False] = {**self.default_in_branch, **action_config[False]}\n",
    "\n",
    "                if pc[action_name][False][\"loss\"]==True:\n",
    "                    pc[action_name][False][\"loss\"]=self.model.compiled_loss\n",
    "        return pc\n",
    "\n",
    "    def _extract_substeps_varialbe_arrays(self, config: Dict[str, Any]) -> None:\n",
    "        # keep varaibles from variable attribute from config file\n",
    "        self.model._included_variables = {}\n",
    "        # keep varaibles from excluded_variable attribute from config file\n",
    "        self.model._excluded_variables = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            self.model._included_variables[action_name]={}\n",
    "            self.model._excluded_variables[action_name]={}\n",
    "\n",
    "            if True in action_config and action_config[True][\"variables\"]:\n",
    "                get_vars = action_config[True][\"variables\"]\n",
    "                bind(self.model, get_vars)\n",
    "                self.model._included_variables[action_name][True] = getattr(self.model, get_vars.__name__)()\n",
    "            if False in action_config and action_config[False][\"variables\"]:\n",
    "                    get_vars = action_config[False][\"variables\"]\n",
    "                    bind(self.model, get_vars)\n",
    "                    self.model._included_variables[action_name][False] = getattr(self.model, get_vars.__name__)()\n",
    "                    \n",
    "            if True in action_config and action_config[True][\"excluded_variables\"]:\n",
    "                get_vars = action_config[True][\"excluded_variables\"]\n",
    "                bind(self.model, get_vars)\n",
    "                exclude_variables_names = [x.name for x in getattr(self.model, get_vars.__name__)()]\n",
    "                self.model._excluded_variables[action_name][True] = [x for x in self.model.trainable_variables if x.name not in exclude_variables_names]\n",
    "\n",
    "            if False in action_config and action_config[False][\"excluded_variables\"]:\n",
    "                get_vars = action_config[False][\"excluded_variables\"]\n",
    "                bind(self.model, get_vars)\n",
    "                exclude_variables_names = [x.name for x in getattr(self.model, get_vars.__name__)()]\n",
    "                self.model._excluded_variables[action_name][False] = [x for x in self.model.trainable_variables if x.name not in exclude_variables_names]\n",
    "\n",
    "    def reinitialize(self):\n",
    "        tf.print(\"\\n-------------------Reinitializatoin-------------------\\n\")\n",
    "        self.model.reinitialize()\n",
    "        self.reinit_history = []\n",
    "        self.reinit_epochs = 0\n",
    "        self.reinit_batches = 0\n",
    "\n",
    "    def _bind_reinitialization(self, reinit_config: Dict[str, Any]) -> None:\n",
    "\n",
    "        bind(self, self.reinit_config[\"cond\"], \"reinitialize_cond\")\n",
    "\n",
    "        if reinit_config[\"reinit_fn\"]:\n",
    "            Warning(\"binding reinitalization method from reinit_config to the model instance.\")\n",
    "            bind(self.model, reinit_config[\"reinit_fn\"], \"reinitialize\")\n",
    "        else:\n",
    "            if not callable(getattr(self.model.__class__, \"reinitialize\", None)):\n",
    "                raise ReferenceError(\"model has no self.reinitialize() method\")\n",
    "            Warning(\"reinit_fn from reinit_config is none: using self.reinitialize() from the model instance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NALURegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, reg_coef=0.1):\n",
    "        self.reg_coef = reg_coef\n",
    "\n",
    "    def __call__(self, var: List[tf.Variable]) -> tf.Tensor:\n",
    "        return self.reg_coef * tf.add_n(\n",
    "            [\n",
    "                tf.reduce_mean(tf.math.maximum(tf.math.minimum(-v, v) + 20, 0))\n",
    "                for v in var\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"reg_coef\": float(self.reg_coef)}\n",
    "\n",
    "class NALU(tf.keras.layers.Layer):\n",
    "\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        units: int,\n",
    "        regularizer: types.Regularizer = NALURegularizer(reg_coef=0.05),\n",
    "        clipping: float = 20,\n",
    "        w_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=1.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        m_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=-1.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        g_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=0.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(NALU, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.reg_fn = regularizer\n",
    "        self.clipping = clipping\n",
    "\n",
    "        self.w_initializer = w_initializer\n",
    "        self.m_initializer = m_initializer\n",
    "        self.g_initializer = g_initializer\n",
    "\n",
    "        self.gate_as_vector = True\n",
    "        self.force_operation = None\n",
    "        self.weights_separation = True\n",
    "        self.input_gate_dependance = False\n",
    "        self.initializer = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # action variables\n",
    "        self.w_hat = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.w_initializer,\n",
    "            trainable = True,\n",
    "            name = \"w\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        self.m_hat = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.m_initializer,\n",
    "            trainable = True,\n",
    "            name = \"m\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        self.w_hat_prime = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.w_initializer,\n",
    "            trainable = True,\n",
    "            name = \"w_prime\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        self.m_hat_prime = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.m_initializer,\n",
    "            trainable = True,\n",
    "            name = \"m_prime\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        # gating varaible\n",
    "        self.g = self.add_weight (\n",
    "            shape = (self.units, ),\n",
    "            initializer = self.g_initializer,\n",
    "            trainable = True,\n",
    "            name = \"g\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def get_reg_loss(self):\n",
    "        var_list = [self.w_hat, self.m_hat, self.g]\n",
    "        if self.weights_separation:\n",
    "            var_list += [self.w_hat_prime, self.m_hat_prime]\n",
    "        return self.reg_fn(var_list)\n",
    "\n",
    "    def call(self, input):\n",
    "        eps = 1e-7\n",
    "        w1 = tf.math.tanh(self.w_hat) * tf.math.sigmoid(self.m_hat)\n",
    "        w2 = tf.math.tanh(self.w_hat_prime) * tf.math.sigmoid(self.m_hat_prime)\n",
    "        a1 = tf.matmul(input, w1)\n",
    "\n",
    "        m1 = tf.math.exp(tf.minimum(tf.matmul(tf.math.log(tf.maximum(tf.math.abs(input), eps)),w2), self.clipping))\n",
    "        \n",
    "        # sign\n",
    "        w1s = tf.math.abs(tf.reshape(w2, [-1]))\n",
    "        xs = tf.concat([input] * w1.shape[1], axis=1)\n",
    "        xs = tf.reshape(xs, shape=[-1, w1.shape[0] * w1.shape[1]])\n",
    "        sgn = tf.sign(xs) * w1s + (1 - w1s)\n",
    "        sgn = tf.reshape(sgn, shape=[-1, w1.shape[1], w1.shape[0]])\n",
    "        ms = tf.math.reduce_prod(sgn, axis=2)\n",
    "        \n",
    "        self.add_loss(lambda: self.get_reg_loss())\n",
    "        g1 = tf.math.sigmoid(self.g)\n",
    "        return g1 * a1 + (1 - g1) * m1 * tf.clip_by_value(ms, -1, 1)\n",
    "\n",
    "    def reinitialize(self):\n",
    "        self.g.assign(self.g_initializer(self.g.shape))\n",
    "        self.w_hat.assign(self.w_initializer(self.w_hat.shape))\n",
    "        self.m_hat.assign(self.m_initializer(self.m_hat.shape))\n",
    "        self.w_hat_prime.assign(self.w_initializer(self.w_hat_prime.shape))\n",
    "        self.m_hat_prime.assign(self.m_initializer(self.m_hat_prime.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------MASTER STEP-------------------\n",
      "\n",
      "@tf.function\n",
      "def train_step(self, data):\n",
      "    loss = {'loss_gate' : tf.cond(self.control_variables['gate'], lambda: self.gate_on(data), lambda: self.gate_off(data)),'loss_delay' : tf.cond(self.control_variables['delay'], lambda: self.delay_on(data), lambda: 0.0)}\n",
      "    metrics = {m.name : m.result() for m in self.metrics}\n",
      "    # if \"loss\" not in metrics and self.loss in self.metrics_names:\n",
      "    #     metrics[\"loss\"] = metrics.pop(self.loss)\n",
      "    control_states = {\n",
      "            control_name: tf.cond(\n",
      "                control_value,\n",
      "                lambda: tf.constant(True),\n",
      "                lambda: tf.constant(False),\n",
      "            )\n",
      "            for control_name, control_value in self.control_variables.items()\n",
      "    }\n",
      "    \n",
      "    return {**loss, **metrics, **control_states}\n",
      "\n",
      "-------------------SLAVE STEPS-------------------\n",
      "\n",
      "-------------------gate_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      "        \n",
      "        for g in self._excluded_variables['gate'][True]:\n",
      "            tape.watch(g)\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value =  loss(y, logits)\n",
      "        \n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.1, 0.1) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_off-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_off(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      "        \n",
      "        for g in self._included_variables['gate'][False]:\n",
      "            tape.watch(g)\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value =  loss(y, logits)\n",
      "        \n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.1, 0.1) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------delay_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def delay_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "        \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = tf.constant(0, dtype=tf.float32) * loss(y, logits)\n",
      "        \n",
      "        loss_value += tf.math.reduce_sum(self.losses)\n",
      "        \n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "Epoch 1/40\n",
      "1000/1000 [==============================] - 11s 4ms/step - loss_gate: 4.1912 - loss_delay: 0.0000e+00 - loss: 4.1926 - mae: 1.4048 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 2/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 1.5067 - loss_delay: 0.0000e+00 - loss: 1.5069 - mae: 0.8496 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 3/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 1.2168 - loss_delay: 0.0000e+00 - loss: 1.2168 - mae: 0.7448 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 4/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 1.1074 - loss_delay: 0.0000e+00 - loss: 1.1075 - mae: 0.7042 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 5/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 1.0137 - loss_delay: 0.0000e+00 - loss: 1.0139 - mae: 0.6535 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 6/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 0.9587 - loss_delay: 0.0000e+00 - loss: 0.9588 - mae: 0.6200 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 7/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 0.8620 - loss_delay: 0.0000e+00 - loss: 0.8620 - mae: 0.5999 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 8/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 0.8355 - loss_delay: 0.0000e+00 - loss: 0.8355 - mae: 0.5879 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 9/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 0.8288 - loss_delay: 0.0000e+00 - loss: 0.8288 - mae: 0.5834 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 10/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 0.8313 - loss_delay: 0.0000e+00 - loss: 0.8313 - mae: 0.5833 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 11/40\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss_gate: 0.9496 - loss_delay: 38.6478 - loss: 0.9471 - mae: 0.6364 - gate: 0.7992 - delay: 1.0000\n",
      "\n",
      "-------------------Reinitializatoin-------------------\n",
      "\n",
      "Epoch 12/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 4.3875 - loss_delay: 0.0719 - loss: 4.3874 - mae: 1.4335 - gate: 0.7992 - delay: 9.9900e-04\n",
      "Epoch 13/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 1.0470 - loss_delay: 0.0000e+00 - loss: 1.0477 - mae: 0.6118 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 14/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 0.0818 - loss_delay: 0.0000e+00 - loss: 0.0819 - mae: 0.1736 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 15/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 0.0078 - loss_delay: 0.0000e+00 - loss: 0.0078 - mae: 0.0529 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 16/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 8.1098e-04 - loss_delay: 0.0000e+00 - loss: 8.1119e-04 - mae: 0.0175 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 17/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 2.3010e-04 - loss_delay: 0.0000e+00 - loss: 2.3006e-04 - mae: 0.0097 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 18/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 1.3369e-04 - loss_delay: 0.0000e+00 - loss: 1.3364e-04 - mae: 0.0074 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 19/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 9.4591e-05 - loss_delay: 0.0000e+00 - loss: 9.4556e-05 - mae: 0.0063 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 20/40\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_gate: 7.3822e-05 - loss_delay: 0.0000e+00 - loss: 7.3791e-05 - mae: 0.0055 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 21/40\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss_gate: 6.0527e-05 - loss_delay: 0.0000e+00 - loss: 6.0502e-05 - mae: 0.0050 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 22/40\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss_gate: 5.1528e-05 - loss_delay: 0.0000e+00 - loss: 5.1505e-05 - mae: 0.0046 - gate: 0.7992 - delay: 0.0000e+00\n",
      "Epoch 23/40\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss_gate: 7.6566e-04 - loss_delay: 33.7525 - loss: 7.6422e-04 - mae: 0.0090 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 24/40\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss_gate: 4.9453e-14 - loss_delay: 2.2030 - loss: 4.9454e-14 - mae: 1.1501e-07 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 25/40\n",
      "1000/1000 [==============================] - 8s 8ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 26/40\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 27/40\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 28/40\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 29/40\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 30/40\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 31/40\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 32/40\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 33/40\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 34/40\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 35/40\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 36/40\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 37/40\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 38/40\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 39/40\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n",
      "Epoch 40/40\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss_gate: 3.4752e-14 - loss_delay: 0.0000e+00 - loss: 3.4739e-14 - mae: 9.5870e-08 - gate: 0.7992 - delay: 1.0000\n"
     ]
    }
   ],
   "source": [
    "seed = 105\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_dim),\n",
    "    NALU(3, clipping = 20),\n",
    "    NALU(2, clipping = 20),\n",
    "    NALU(1, clipping = 20)\n",
    "    ])\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=\n",
    "    tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mae\"])\n",
    "\n",
    "def gate_open(self):\n",
    "    return self.batches % 10 < 8\n",
    "\n",
    "def delay_reg(self):\n",
    "    return self.reinit_epochs > 10 and self.reinit_history and self.reinit_history[-1] < 1.0\n",
    "    \n",
    "def get_gates_variables(self) -> List[tf.Variable]:\n",
    "    return [l.g for l in self.layers if isinstance(l, NALU)]\n",
    "    \n",
    "config = {\n",
    "    \"gate\": {\n",
    "        \"cond\": gate_open,\n",
    "        True: {\n",
    "            \"clipping\": (-0.1, 0.1),\n",
    "            \"excluded_variables\": get_gates_variables\n",
    "        },\n",
    "        False: {\n",
    "            \"clipping\": (-0.1, 0.1),\n",
    "            \"variables\": get_gates_variables,\n",
    "        }\n",
    "    },\n",
    "    \"delay\": {\n",
    "        \"cond\": delay_reg,\n",
    "        True: {\n",
    "            \"loss\": False,\n",
    "            \"regularize\": True\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def reinit_cond(self):\n",
    "    split_index = len(self.reinit_history) // 2\n",
    "    return (len(self.reinit_history) > 10000) and (tf.math.reduce_mean(self.reinit_history[split_index:]) > 0.7)\n",
    "\n",
    "def reinitialize_fn(self):\n",
    "    for l in self.layers:\n",
    "        if isinstance(l, NALU):\n",
    "            l.reinitialize()\n",
    "\n",
    "reinit_config = {\n",
    "    \"cond\": reinit_cond,\n",
    "    \"reinit_fn\": reinitialize_fn\n",
    "}\n",
    "\n",
    "\n",
    "seed = 1\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "lcc = LoopControlerCallback(config, reinit_config, verbose=1)\n",
    "# start training\n",
    "history = model.fit(data_dp, epochs = 40, verbose = 1,\n",
    "    callbacks=[lcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b4ff1917ad7fe489d902c3041fcac2959f3e2431a3acc02677f15f306757aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
