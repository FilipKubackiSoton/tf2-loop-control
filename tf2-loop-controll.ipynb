{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow2 training loop control using default *tf.fit(...)* function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Descripton\n",
    "\n",
    "Up to now custom training loop in Tensorflow2 requires writing two lops:\n",
    "1. loop iterating through epochs \n",
    "2. loop iterating through batches \n",
    "\n",
    "Then all castom training precudere will have to be implemented in these double-loop block of code. It's neither elegant nor robust due to the missing advanced features of *tf.fit(...)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 11:23:25.315566: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-10 11:23:25.315611: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind(instance, func, as_name=None):\n",
    "    \"\"\"\n",
    "    Bind the function *func* to *instance*, with either provided name *as_name*\n",
    "    or the existing name of *func*. The provided *func* should accept the \n",
    "    instance as the first argument, i.e. \"self\".\n",
    "    \"\"\"\n",
    "    if as_name is None:\n",
    "        as_name = func.__name__\n",
    "    bound_method = func.__get__(instance, instance.__class__)\n",
    "    setattr(instance, as_name, bound_method)\n",
    "    return bound_method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopControlerCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, config: int, default_in_branch: Dict[str, Any] = None, verbose: bool = True, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super(LoopControlerCallback, self).__init__(*args, **kwargs)\n",
    "        self.default_in_branch: Dict[str, Any] = default_in_branch if default_in_branch else {\n",
    "                \"loss\": None, \n",
    "                \"clipping\": None,\n",
    "                \"variables\": None,\n",
    "                \"exclud_var\": None,\n",
    "            }\n",
    "        self.config: Dict[str, Any] = config\n",
    "        self.verbose: bool = verbose\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"Function called directely before training. It is executed on each call of model.fit with this callback.\n",
    "            Inside the scope of this funciton we can access model on which this callback works: self.model.\n",
    "\n",
    "        Args:\n",
    "            logs (_type_, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        # meta attributes for building conditions\n",
    "        self.epochs: int = 0\n",
    "        self.batches: int = 0\n",
    "        self.batches_in_epoch: int = 0\n",
    "        self.last_loss: int = 0.0\n",
    "        self.history: List[Any] = []\n",
    "        \n",
    "        # if loss in default branch is None, then use compiled loss\n",
    "        if not self.default_in_branch[\"loss\"]:\n",
    "            self.default_in_branch[\"loss\"] = self.model.compiled_loss\n",
    "\n",
    "        # extend config with validation step\n",
    "        self.config = self._extend_config(self.config)\n",
    "        # initiate control variables\n",
    "        self._init_cv(self.config)\n",
    "        # bind control variables to model\n",
    "        self._bind_cv(self.config)\n",
    "        # bind master train_step to model\n",
    "        self._bind_master_step(self.config)\n",
    "        # bind placeholders for list of variables for slave steps\n",
    "        self._bind_slaves_steps_dif_variables(self.config)\n",
    "        # bind slave train_steps to model\n",
    "        self._bin_slaves_steps()\n",
    "\n",
    "\n",
    "    def _extend_config(self, config: Dict[str, Any]) -> None:\n",
    "        \"\"\"Extend and validate config file. Fill missing fields based on the default_in_branch.\n",
    "\n",
    "        Args:\n",
    "            config (Dict[str, Any]): Configuration to control model training\n",
    "        \"\"\"\n",
    "\n",
    "        def validate_action_config(action_name: str, action_config: Dict[str, Any]) -> None:\n",
    "            \"\"\"Validate model training configuration.\n",
    "\n",
    "            Args:\n",
    "                action_name (str): name of the action slave train step\n",
    "                action_config (Dict[str, Any]): configuration of the action slave train step\n",
    "\n",
    "            Raises:\n",
    "                ValueError: Missing controlable cond\n",
    "                ValueError: Missing branch configuration for true/false after cond\n",
    "            \"\"\"\n",
    "\n",
    "            Warning(f\"------Validating Configuration for {action_name}------\")\n",
    "            if action_config == {}:\n",
    "                Warning(\n",
    "                    f\"{action_name} has empty body. Condition and False or True branch must be implemented.\\n It's ignored in furhter computations\"\n",
    "                )\n",
    "            if (True not in action_config) and (False not in action_config):\n",
    "                raise ValueError(\n",
    "                    f\"{action_name} has no False or True branch implemented\"\n",
    "                )\n",
    "            if \"cond\" not in action_config:\n",
    "                raise ValueError(f\"{action_name} has no condition implemented.\")\n",
    "\n",
    "        pc = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            validate_action_config(action_name, action_config)\n",
    "            pc[action_name] = {\"cond\": action_config[\"cond\"]}\n",
    "            if True in action_config:\n",
    "                pc[action_name][True] = {**self.default_in_branch, **action_config[True]}\n",
    "            if False in action_config:\n",
    "                pc[action_name][False] = {**self.default_in_branch, **action_config[False]}\n",
    "\n",
    "        return pc\n",
    "\n",
    "    def _get_cv_name(self, action_name: str) -> str:\n",
    "        \"\"\"Get control variable from action name\n",
    "\n",
    "        Args:\n",
    "            action_name (str): name of the action slave train step\n",
    "\n",
    "        Returns:\n",
    "            str: converted name of the action slave train step\n",
    "        \"\"\"\n",
    "        return f\"{action_name}_cv\"\n",
    "\n",
    "    def _get_cc_names(self, condition_name: str) -> Tuple[str, str]:\n",
    "        return (f\"{condition_name}_off\", f\"{condition_name}_on\")\n",
    "\n",
    "    def _init_cv(self, config) -> None:\n",
    "        self.model.cv_names = []\n",
    "        for cv_name in config.keys():\n",
    "            setattr(self.model, cv_name, tf.Variable(False, trainable=False))\n",
    "            self.model.cv_names.append(cv_name)\n",
    "\n",
    "    def _bind_cv(self, config) -> None:\n",
    "        self.c_conds = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            name = self._get_cv_name(action_name)\n",
    "            bind(self, action_config[\"cond\"], name)\n",
    "            self.c_conds[action_name] = name\n",
    "\n",
    "    def _bind_master_step(self, config) -> None:\n",
    "        lscope = locals()\n",
    "        def _get_losses(config: Dict[str, Any]) -> str:\n",
    "            def _substeps_condition_execution(name: str, conf: Dict[str, Any], on: bool) -> str:\n",
    "                if on:\n",
    "                    return f\"self.{name}_on(data)\" if True in conf else \"0.0\"\n",
    "                else:\n",
    "                    return f\"self.{name}_off(data)\" if False in conf else \"0.0\"\n",
    "\n",
    "            return \"{\" + \",\".join([\n",
    "                        f\"'loss_{an}' : tf.cond(self.{an}, lambda: {_substeps_condition_execution(an, ac, True)}, lambda: {_substeps_condition_execution(an, ac, False)})\"\n",
    "                        for an, ac in config.items()\n",
    "                    ]) + \"}\"\n",
    "\n",
    "        lscope = locals()\n",
    "        function_body = \"\"\"\n",
    "@tf.function\n",
    "def train_step(self, data):\n",
    "    loss = {losses_config}\n",
    "    metrics = {{m.name : m.result() for m in self.metrics}}\n",
    "    \n",
    "    control_states = {{\n",
    "            c_name: tf.cond(\n",
    "                getattr(self, c_name),\n",
    "                lambda: tf.constant(True),\n",
    "                lambda: tf.constant(False),\n",
    "            )\n",
    "            for c_name in self.cv_names\n",
    "    }}\n",
    "    \n",
    "    return {{**loss, **metrics, **control_states}}\n",
    "\"\"\".format(\n",
    "            **{\"losses_config\": _get_losses(config)}\n",
    "        )\n",
    "        if self.verbose:\n",
    "            print(\"-------------------MASTER STEP-------------------\")\n",
    "            print(function_body)\n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[\"train_step\"])\n",
    "\n",
    "    def _get_name_for_sleve_step_variables(self, name, on = None):\n",
    "        if on == None:\n",
    "            return f\"{name}_vars\"\n",
    "        return f\"{name}_{'on' if on else 'off'}_vars\"\n",
    "\n",
    "    def _bind_slaves_steps_dif_variables(self, config):\n",
    "        for action_name, action_config in config.items():\n",
    "            if action_config[True] and \"variables\" in action_config[True] and action_config[True][\"variables\"]:\n",
    "                # we have to bind None because variables before trainign are not initialised\n",
    "                setattr(self.model, self._get_name_for_sleve_step_variables(action_name, True), action_config[True][\"variables\"])\n",
    "\n",
    "\n",
    "            if False in action_config and \"variables\" in action_config[False] and action_config[False][\"variables\"]:\n",
    "                # we have to bind None because variables before trainign are not initialised\n",
    "                setattr(self.model, self._get_name_for_sleve_step_variables(action_name, False), action_config[False][\"variables\"])\n",
    "\n",
    "    def _bin_slaves_steps(self) -> None:\n",
    "        if self.verbose:\n",
    "            print(\"-------------------SLAVE STEPS-------------------\\n\")\n",
    "        for action_name, action_config in self.config.items():\n",
    "            off_step_name, on_step_name = self._get_cc_names(action_name)\n",
    "            if True in action_config:\n",
    "                self._bind_slave_step(on_step_name, action_config[True])\n",
    "            if False in action_config:\n",
    "                self._bind_slave_step(off_step_name, action_config[False])\n",
    "\n",
    "    def _bind_slave_step(self, fn_name: str, fn_config: Dict[str, Any]) -> None:\n",
    "        lscope = {\n",
    "            **locals(), \n",
    "            **fn_config,\n",
    "            **{\"vars_control_name\": self._get_name_for_sleve_step_variables(fn_name)}\n",
    "            }\n",
    "        # print(getattr(self.model, vars_control_name)())\n",
    "        function_body = \"\"\"\n",
    "@tf.function\n",
    "def {fn_name}(self, data):\n",
    "    x, y = data\n",
    "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
    "        logits = self(x, training=True)\n",
    "        loss_value = loss(y, logits)\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip({clipping_grads}, tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "    return loss_value\n",
    "\"\"\".format(**{\n",
    "    \"fn_name\": fn_name,\n",
    "    \"clipping_grads\": \"[tf.clip_by_value(g, {clip_low}, {clip_high}) for g in grads]\".format(**{\"clip_low\": fn_config[\"clipping\"][0], \"clip_high\": fn_config[\"clipping\"][1]}) if fn_config[\"clipping\"] else \"grads\",\n",
    "    \"variables\": \"if not getattr(self.model, vars_control_name):\\n      getattr(self.model, vars_control_name)=variables()\\n\" if fn_config[\"variables\"] else \"\",\n",
    "    \"test\" : self._get_name_for_sleve_step_variables(fn_name)\n",
    "\n",
    "})\n",
    "        if self.verbose:\n",
    "            print(f\"-------------------{fn_name}-------------------\")\n",
    "            print(fn_config[\"variables\"])\n",
    "            print(function_body)\n",
    "            \n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[fn_name])\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs) -> None:\n",
    "        self.epochs += 1\n",
    "        \"\"\"Control gating variable from the level of callback which can work on epoch/batch level.\"\"\"\n",
    "        # tf.variable.assign is different than tf.variable = <sth>. The second option is compiled to static\n",
    "        # value in TF graph of computation as the result of @tf.function decorators in LoopControlableModel\n",
    "        for control_name, control_function_name in self.c_conds.items():\n",
    "            getattr(self.model, control_name).assign(\n",
    "                getattr(self, control_function_name)()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------MASTER STEP-------------------\n",
      "\n",
      "@tf.function\n",
      "def train_step(self, data):\n",
      "    loss = {'loss_gate' : tf.cond(self.gate, lambda: self.gate_on(data), lambda: self.gate_off(data)),'loss_delay' : tf.cond(self.delay, lambda: self.delay_on(data), lambda: 0.0)}\n",
      "    metrics = {m.name : m.result() for m in self.metrics}\n",
      "    \n",
      "    control_states = {\n",
      "            c_name: tf.cond(\n",
      "                getattr(self, c_name),\n",
      "                lambda: tf.constant(True),\n",
      "                lambda: tf.constant(False),\n",
      "            )\n",
      "            for c_name in self.cv_names\n",
      "    }\n",
      "    \n",
      "    return {**loss, **metrics, **control_states}\n",
      "\n",
      "-------------------SLAVE STEPS-------------------\n",
      "\n",
      "-------------------gate_on-------------------\n",
      "<function dummy_vars at 0x7f51373d11f0>\n",
      "\n",
      "@tf.function\n",
      "def gate_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.2, 0.3) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_off-------------------\n",
      "None\n",
      "\n",
      "@tf.function\n",
      "def gate_off(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------delay_on-------------------\n",
      "None\n",
      "\n",
      "@tf.function\n",
      "def delay_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 2s 3ms/step - loss_gate: 0.3245 - loss_delay: 0.0000e+00 - loss: 0.0000e+00 - mae: 0.3225 - gate: 1.0000 - delay: 0.0000e+00\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_gate: 0.1189 - loss_delay: 0.0000e+00 - loss: 0.1165 - mae: 0.2827 - gate: 0.0000e+00 - delay: 0.0000e+00\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_gate: 0.2725 - loss_delay: 0.0000e+00 - loss: 0.0000e+00 - mae: 0.2699 - gate: 1.0000 - delay: 0.0000e+00\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_gate: 0.0982 - loss_delay: 0.0000e+00 - loss: 0.0967 - mae: 0.2620 - gate: 0.0000e+00 - delay: 0.0000e+00\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_gate: 0.2598 - loss_delay: 0.0000e+00 - loss: 0.0000e+00 - mae: 0.2580 - gate: 1.0000 - delay: 0.0000e+00\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss_gate: 0.0898 - loss_delay: 0.0882 - loss: 0.0891 - mae: 0.2536 - gate: 0.0000e+00 - delay: 1.0000\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_gate: 0.2535 - loss_delay: 0.0863 - loss: 0.0000e+00 - mae: 0.2515 - gate: 1.0000 - delay: 1.0000\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss_gate: 0.0867 - loss_delay: 0.0851 - loss: 0.0863 - mae: 0.2508 - gate: 0.0000e+00 - delay: 1.0000\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss_gate: 0.2525 - loss_delay: 0.0853 - loss: 0.0000e+00 - mae: 0.2507 - gate: 1.0000 - delay: 1.0000\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_gate: 0.0864 - loss_delay: 0.0848 - loss: 0.0860 - mae: 0.2507 - gate: 0.0000e+00 - delay: 1.0000\n"
     ]
    }
   ],
   "source": [
    "DATASET_SIZE, INPUT_SIZE, OUTPUT_SIZE = 1000, 2, 1\n",
    "BATCH_SIZE = 64\n",
    "data = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform((DATASET_SIZE, INPUT_SIZE)), tf.random.uniform((DATASET_SIZE, OUTPUT_SIZE)))\n",
    "    ).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(OUTPUT_SIZE)])\n",
    "# compile model\n",
    "model.compile(optimizer=\n",
    "    tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "def gate_config(self):\n",
    "    return self.epochs % 2 == 1\n",
    "\n",
    "def delay_config(self):\n",
    "    return self.epochs > 5\n",
    "\n",
    "def dummy_vars(self,):\n",
    "    return [self.variables[0]]\n",
    "\n",
    "config = {\n",
    "    \"gate\": {\n",
    "        \"cond\": gate_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanAbsoluteError(),\n",
    "            \"clipping\": (-0.2, 0.3),\n",
    "            \"variables\": dummy_vars\n",
    "        },\n",
    "        False: {}\n",
    "        # False: {\n",
    "        #     # \"loss\": tf.keras.losses.MeanAbsoluteError(), \n",
    "        # }\n",
    "    },\n",
    "    \"delay\": {\n",
    "        \"cond\": delay_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanSquaredError(), \n",
    "        },\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "lcc = LoopControlerCallback(config)\n",
    "# start training\n",
    "history = model.fit(data, epochs = 10, verbose = 1,\n",
    "    callbacks=[lcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>()>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gate_on_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gate': {'cond': <function __main__.gate_config(self)>,\n",
       "  True: {'loss': <keras.losses.MeanSquaredError at 0x7f4c4d08da00>,\n",
       "   'clipping': None,\n",
       "   'var': None,\n",
       "   'exclud_var': None},\n",
       "  False: {'loss': <keras.losses.MeanAbsoluteError at 0x7f4c4d08d490>,\n",
       "   'clipping': None,\n",
       "   'var': None,\n",
       "   'exclud_var': None}},\n",
       " 'delay': {'cond': <function __main__.delay_config(self)>,\n",
       "  True: {'loss': <keras.losses.MeanSquaredError at 0x7f4c4d08ddf0>,\n",
       "   'clipping': None,\n",
       "   'var': None,\n",
       "   'exclud_var': None},\n",
       "  False: None},\n",
       " 'test': {'cond': <function __main__.delay_config(self)>,\n",
       "  True: {'loss': 'self.compiled_loss',\n",
       "   'clipping': None,\n",
       "   'var': None,\n",
       "   'exclud_var': None},\n",
       "  False: None}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"gate\": {\n",
    "        \"cond\": gate_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanSquaredError(), \n",
    "        },\n",
    "        False: {\n",
    "            \"loss\": tf.keras.losses.MeanAbsoluteError(), \n",
    "        }\n",
    "    },\n",
    "    \"delay\": {\n",
    "        \"cond\": delay_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanSquaredError(), \n",
    "        },\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"cond\": delay_config,\n",
    "        True: {}\n",
    "    },\n",
    "    # \"test2\": {\n",
    "    #     \"cond\": delay_config,\n",
    "    # },\n",
    "\n",
    "}\n",
    "\n",
    "def validate_action_config(action_name, action_config) -> bool:\n",
    "        Warning(f\"------Validating Configuration for {action_name}------\")\n",
    "        if action_config == {}:\n",
    "            Warning(f\"{action_name} has empty body. Condition and False or True branch must be implemented.\\n It's ignored in furhter computations\")\n",
    "        if ((True not in action_config) and (True not in action_config)):\n",
    "            raise ValueError(f\"{action_name} has no False or True branch implemented\")\n",
    "        if \"cond\" not in action_config:\n",
    "            raise ValueError(f\"{action_name} has no condition implemented.\")\n",
    "        return True\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "def parse_config(config, default_in_branch = None):\n",
    "    if not default_in_branch:\n",
    "        default_in_branch = {\n",
    "            \"loss\" : \"self.compiled_loss\",\n",
    "            \"clipping\" : None, \n",
    "            \"var\": None,\n",
    "            \"exclud_var\": None,\n",
    "        }\n",
    "\n",
    "    pc = {}\n",
    "    for action_name, action_config in config.items():\n",
    "        _ = validate_action_config(action_name, action_config)\n",
    "        false_branch, true_branch = {}, {}\n",
    "        true_branch = {**default_in_branch,**action_config[True]} if True in action_config else None\n",
    "        false_branch = {**default_in_branch,**action_config[False]} if False in action_config else None\n",
    "        pc[action_name] = {\"cond\": action_config[\"cond\"], True: true_branch, False: false_branch}\n",
    "\n",
    "    return pc \n",
    "            \n",
    "parse_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False not in {False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import signature\n",
    "\n",
    "def foo(x: int = 1, y: int = 2) -> bool:\n",
    "    return x > y\n",
    "sig = signature(foo)\n",
    "sig.return_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gate_on(self, data):\n",
    "    if not self.gate_var:\n",
    "\tgetattr(self, self._get_name_for_sleve_step_variables(gate_on))=variables\n",
    "\n",
    "    x, y = data\n",
    "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
    "        logits = self(x, training=True)\n",
    "        loss_value = loss(y, logits)\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.2, 0.3) for g in grads], tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "class B:\n",
    "    def __init__(self) -> None:\n",
    "        self.x = 2\n",
    "    \n",
    "    def boo(self):\n",
    "        \n",
    "        bb = tf.keras.losses.MeanAbsoluteError()\n",
    "        adict = locals()\n",
    "        # print(adict)\n",
    "        function_body = f\"\"\"\n",
    "def train(self):\n",
    "    print(bb([1,2,3], [3,4,5]))\n",
    "    return self.x\n",
    "\"\"\"\n",
    "        exec(function_body,{**globals(), **adict}, adict)\n",
    "        # print(adict)\n",
    "        # time.sleep(0.1)\n",
    "        bind(self, adict[\"train\"])\n",
    "        print(self.train())\n",
    "        # bind(self, test3)\n",
    "        # bind(self, test)\n",
    "        return True\n",
    "\n",
    "b = B()\n",
    "b.boo()\n",
    "# self.train_substep()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b4ff1917ad7fe489d902c3041fcac2959f3e2431a3acc02677f15f306757aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
