{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow2 training loop control using default *tf.fit(...)* function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Descripton\n",
    "\n",
    "Up to now custom training loop in Tensorflow2 requires writing two lops:\n",
    "1. loop iterating through epochs \n",
    "2. loop iterating through batches \n",
    "\n",
    "Then all castom training precudere will have to be implemented in these double-loop block of code. It's neither elegant nor robust due to the missing advanced features of *tf.fit(...)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 17:51:01.553942: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 17:51:02.499179: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-28 17:51:02.499231: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-28 17:51:07.721242: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 17:51:07.722522: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 17:51:07.722565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind(instance, func, as_name=None):\n",
    "    \"\"\"\n",
    "    Bind the function *func* to *instance*, with either provided name *as_name*\n",
    "    or the existing name of *func*. The provided *func* should accept the \n",
    "    instance as the first argument, i.e. \"self\".\n",
    "    \"\"\"\n",
    "    if as_name is None:\n",
    "        as_name = func.__name__\n",
    "    bound_method = func.__get__(instance, instance.__class__)\n",
    "    setattr(instance, as_name, bound_method)\n",
    "    return bound_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE, INPUT_SIZE, OUTPUT_SIZE = 1000, 2, 1\n",
    "BATCH_SIZE = 64\n",
    "data = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform((DATASET_SIZE, INPUT_SIZE)), tf.random.uniform((DATASET_SIZE, OUTPUT_SIZE)))\n",
    "    ).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopControlerCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, \n",
    "        config: int, \n",
    "        reinit_config: Dict[str, Any] = None, \n",
    "        default_in_branch: Dict[str, Any] = {\n",
    "                \"loss\": True, \n",
    "                \"regularize\": None, \n",
    "                \"clipping\": None,\n",
    "                \"variables\": None,\n",
    "                \"excluded_variables\": None,\n",
    "            },\n",
    "            verbose: bool = True,\n",
    "            *args, **kwargs\n",
    "    ) -> None:\n",
    "        super(LoopControlerCallback, self).__init__(*args, **kwargs)\n",
    "        self.default_in_branch: Dict[str, Any] = default_in_branch\n",
    "        self.config: Dict[str, Any] = config\n",
    "        self.reinit_config = reinit_config\n",
    "        self.verbose: bool = verbose\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"Function called directely before training. It is executed on each call of model.fit with this callback.\n",
    "            Inside the scope of this funciton we can access model on which this callback works: self.model.\n",
    "\n",
    "        Args:\n",
    "            logs (_type_, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        # meta attributes for building conditions\n",
    "        self.epochs: int = 0\n",
    "        self.batches: int = 0\n",
    "        self.batches_in_epoch: int = 0\n",
    "        self.last_loss: int = 0.0\n",
    "        self.history: List[Any] = []\n",
    "\n",
    "        # meta attributes for reinitialization handeling\n",
    "        self.reinit_history: List[Any] = []\n",
    "        self.reinit_current_history: List[Any] = []\n",
    "        \n",
    "        # extend config with validation step\n",
    "        self.config = self._extend_config(self.config)\n",
    "\n",
    "        # assign active and exluded variables for slave steps\n",
    "        self._extract_substeps_varialbe_arrays(self.config)\n",
    "\n",
    "        # bind control variables and control conditions\n",
    "        self._bind_controlers(self.config)\n",
    "        \n",
    "        # bind master train_step to model\n",
    "        self._bind_master_step(self.config)\n",
    "\n",
    "        # bind slave train_steps to model\n",
    "        self._bind_slaves_steps(self.config)\n",
    "\n",
    "        # bind reinitalization functionality to model only if reinitalizatoin config is provided\n",
    "        if self.reinit_config:\n",
    "            self._bind_reinitialization(self.reinit_config)\n",
    "\n",
    "    def _bind_master_step(self, config: Dict[str, Any]) -> None:\n",
    "        lscope = locals()\n",
    "        def _get_losses(config: Dict[str, Any]) -> str:\n",
    "            def _substeps_condition_execution(name: str, config: Dict[str, Any], on: bool) -> str:\n",
    "                if on:\n",
    "                    return f\"self.{name}_on(data)\" if True in config else \"0.0\"\n",
    "                else:\n",
    "                    return f\"self.{name}_off(data)\" if False in config else \"0.0\"\n",
    "                    \n",
    "            return \"{\" + \",\".join([\n",
    "                        f\"'loss_{an}' : tf.cond(self.control_variables['{an}'], lambda: {_substeps_condition_execution(an, ac, True)}, lambda: {_substeps_condition_execution(an, ac, False)})\"\n",
    "                        for an, ac in config.items()\n",
    "                    ]) + \"}\"\n",
    "\n",
    "        lscope = locals()\n",
    "        function_body = \"\"\"\n",
    "@tf.function\n",
    "def train_step(self, data):\n",
    "    loss = {losses_config}\n",
    "    metrics = {{m.name : m.result() for m in self.metrics}}\n",
    "    if \"loss\" not in metrics and self.loss in self.metrics_names:\n",
    "        metrics[\"loss\"] = metrics.pop(self.loss)\n",
    "    control_states = {{\n",
    "            control_name: tf.cond(\n",
    "                control_value,\n",
    "                lambda: tf.constant(True),\n",
    "                lambda: tf.constant(False),\n",
    "            )\n",
    "            for control_name, control_value in self.control_variables.items()\n",
    "    }}\n",
    "    \n",
    "    return {{**loss, **metrics, **control_states}}\n",
    "\"\"\".format(\n",
    "            **{\"losses_config\": _get_losses(config)}\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\n-------------------MASTER STEP-------------------\")\n",
    "            print(function_body)\n",
    "        \n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[\"train_step\"])\n",
    "\n",
    "    def _bind_slaves_steps(self, config) -> None:\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"-------------------SLAVE STEPS-------------------\\n\")\n",
    "\n",
    "        for action_name, action_config in config.items():\n",
    "            if True in action_config:\n",
    "                self._bind_slave_step(action_name, action_config[True], True)\n",
    "            if False in action_config:\n",
    "                self._bind_slave_step(action_name, action_config[False], False)\n",
    "\n",
    "    \n",
    "    def _bind_slave_step(self, action_name: str, fn_config: Dict[str, Any], branch: bool) -> None:\n",
    "        \n",
    "\n",
    "        lscope = {\n",
    "            **locals(), \n",
    "            **fn_config\n",
    "            }\n",
    "        fn_name = self._get_actoin_step_name(action_name, branch)\n",
    "        if fn_config[\"loss\"]==False:\n",
    "            # dummy error that will be anyway scale by 0 to make graph to compile otherwise\n",
    "            # ---> 15     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)\n",
    "            # ValueError: None values not supported.\n",
    "            #######################################\n",
    "            # OBSERVATION: loss function with y_pred and y_true must be consumed inside the gradient tape scope\n",
    "            #######################################\n",
    "            lscope[\"loss\"] = self.model.compiled_loss\n",
    "\n",
    "        function_body = f\"\"\"\n",
    "@tf.function\n",
    "def {fn_name}(self, data):\n",
    "    x, y = data\n",
    "    with tf.GradientTape(watch_accessed_variables={'False' if fn_config[\"variables\"] or fn_config[\"excluded_variables\"] else 'True'}) as tape:\n",
    "        \"\"\"\n",
    "\n",
    "        if fn_config[\"variables\"] or fn_config[\"excluded_variables\"]:\n",
    "            function_body += f\"\"\"\n",
    "        for g in self.{'_included_variables' if fn_config[\"variables\"] else '_excluded_variables' }['{action_name}'][{branch}]:\n",
    "            tape.watch(g)\n",
    "            \"\"\"\n",
    "        \n",
    "        function_body += f\"\"\"\n",
    "        logits = self(x, training=True)\n",
    "        loss_value = {'tf.constant(0, dtype=tf.float32) *'  if fn_config[\"loss\"]==False else ''} loss(y, logits)\n",
    "        \"\"\"\n",
    "        # loss_value = {'loss(y, logits)' if fn_config[\"loss\"] else 'tf.constant(0, dtype=tf.float32)'}\n",
    "        # loss_value = {'loss(y, logits)' if fn_config[\"loss\"] else 'tf.math.reduce_sum(self.losses)'}\n",
    "\n",
    "        if fn_config[\"regularize\"]:\n",
    "            function_body += f\"\"\"\n",
    "        loss_value += tf.math.reduce_sum(self.losses)\n",
    "        \"\"\"\n",
    "    \n",
    "        function_body += f\"\"\"\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip({{clipping_grads}}, tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "    return loss_value\n",
    "\"\"\".format(**{\n",
    "    \"clipping_grads\": \"[tf.clip_by_value(g, {clip_low}, {clip_high}) for g in grads]\".format(**{\n",
    "        \"clip_low\": fn_config[\"clipping\"][0], \"clip_high\": fn_config[\"clipping\"][1]\n",
    "        }) if fn_config[\"clipping\"] else \"grads\",\n",
    "    \"regularize_loss_add\": \"loss_value += sum(self.losses)\" if fn_config[\"regularize\"]==True else \"\"\n",
    "})\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"-------------------{fn_name}-------------------\")\n",
    "            print(function_body)\n",
    "        \n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[fn_name])\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs) -> None:\n",
    "        self.epochs += 1\n",
    "        \"\"\"Control gating variable from the level of callback which can work on epoch/batch level.\"\"\"\n",
    "        # tf.variable.assign is different than tf.variable = <sth>. The second option is compiled to static\n",
    "        # value in TF graph of computation as the result of @tf.function decorators in LoopControlableModel\n",
    "        for action_name, _ in self.config.items():\n",
    "            self.model.control_variables[action_name].assign(\n",
    "                getattr(self, self.control_conditions[action_name])()\n",
    "            )\n",
    "        \n",
    "        # reinitlaization \n",
    "        if self.reinit_config and self.reinit_config[\"stage\"]==\"epoch\" and self.reinit_config[\"when\"]==\"begin\":\n",
    "            if self.reinit_config[\"cond\"]():\n",
    "                tf.print(\"\\n-------------------Reinitializatoin-------------------\\n\")\n",
    "                self.reinit_history.append(self.reinit_current_history)\n",
    "                self.reinit_current_history = []\n",
    "                self.model.reinitialize()\n",
    "    \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.batches += 1\n",
    "        \"\"\"Control gating variable from the level of callback which can work on epoch/batch level.\"\"\"\n",
    "        # tf.variable.assign is different than tf.variable = <sth>. The second option is compiled to static\n",
    "        # value in TF graph of computation as the result of @tf.function decorators in LoopControlableModel\n",
    "        for action_name, _ in self.config.items():\n",
    "            self.model.control_variables[action_name].assign(\n",
    "                getattr(self, self.control_conditions[action_name])()\n",
    "            )\n",
    "        \n",
    "        self.history.append(logs)\n",
    "        self.reinit_current_history.append(logs)\n",
    "        \n",
    "        # reinitlaization \n",
    "        if self.reinit_config and  self.reinit_config[\"stage\"]==\"batch\" and self.reinit_config[\"when\"]==\"end\":\n",
    "            if self.reinit_config[\"cond\"]():\n",
    "                tf.print(\"\\n-------------------Reinitializatoin-------------------\\n\")\n",
    "                self.reinit_history.append(self.reinit_current_history)\n",
    "                self.reinit_current_history = []\n",
    "                self.model.reinitialize()\n",
    "\n",
    "        \n",
    "    def _get_actoin_step_name(self, action_name: str, branch: bool) -> str:\n",
    "        return f\"{action_name}_on\" if branch else f\"{action_name}_off\"\n",
    "\n",
    "    def _bind_controlers(self, config) -> None:\n",
    "        self.model.control_variables = {}\n",
    "        self.control_conditions = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            self.model.control_variables[action_name] = tf.Variable(False, trainable=False)\n",
    "            condition_function_name = action_name + \"_condition\"\n",
    "            bind(self, action_config[\"cond\"], condition_function_name)\n",
    "            self.control_conditions[action_name] = condition_function_name\n",
    "\n",
    "    def _extend_config(self, config: Dict[str, Any]) -> None:\n",
    "        \"\"\"Extend and validate config file. Fill missing fields based on the default_in_branch.\n",
    "\n",
    "        Args:\n",
    "            config (Dict[str, Any]): Configuration to control model training\n",
    "        \"\"\"\n",
    "\n",
    "        def validate_action_config(action_name: str, action_config: Dict[str, Any]) -> None:\n",
    "            \"\"\"Validate model training configuration.\n",
    "\n",
    "            Args:\n",
    "                action_name (str): name of the action slave train step\n",
    "                action_config (Dict[str, Any]): configuration of the action slave train step\n",
    "\n",
    "            Raises:\n",
    "                ValueError: Missing controlable cond\n",
    "                ValueError: Missing branch configuration for true/false after cond\n",
    "            \"\"\"\n",
    "\n",
    "            Warning(f\"------Validating Configuration for {action_name}------\")\n",
    "            if action_config == {}:\n",
    "                Warning(\n",
    "                    f\"{action_name} has empty body. Condition and False or True branch must be implemented.\\n It's ignored in furhter computations\"\n",
    "                )\n",
    "            if (True not in action_config) and (False not in action_config):\n",
    "                raise ValueError(\n",
    "                    f\"{action_name} has no False or True branch implemented\"\n",
    "                )\n",
    "            if \"cond\" not in action_config:\n",
    "                raise ValueError(f\"{action_name} has no condition implemented.\")\n",
    "            \n",
    "            # if \"loss\" in action_config and action_config[\"loss\"]==None and \"regularize\" not in action_config:\n",
    "            #     raise ValueError(f\"{action_name} has both loss None and regularize None what makes loss error const zero.\")\n",
    "\n",
    "\n",
    "        class DummyZeroLoss(tf.keras.losses.Loss):\n",
    "            def call(self, y_true, _):\n",
    "                return tf.reduce_mean(y_true - y_true, axis=-1)\n",
    "            \n",
    "        # if loss in default branch is None, then use compiled loss\n",
    "        if self.default_in_branch[\"loss\"]==True:\n",
    "            self.default_in_branch[\"loss\"] = self.model.compiled_loss\n",
    "\n",
    "        pc = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            validate_action_config(action_name, action_config)\n",
    "            pc[action_name] = {\"cond\": action_config[\"cond\"]}\n",
    "            if True in action_config:\n",
    "                pc[action_name][True] = {**self.default_in_branch, **action_config[True]}\n",
    "                if pc[action_name][True][\"loss\"]==True:\n",
    "                    pc[action_name][True][\"loss\"]=self.model.compiled_loss\n",
    "\n",
    "            if False in action_config:\n",
    "                pc[action_name][False] = {**self.default_in_branch, **action_config[False]}\n",
    "\n",
    "                if pc[action_name][False][\"loss\"]==True:\n",
    "                    pc[action_name][False][\"loss\"]=self.model.compiled_loss\n",
    "        return pc\n",
    "\n",
    "    def _extract_substeps_varialbe_arrays(self, config: Dict[str, Any]) -> None:\n",
    "        # keep varaibles from variable attribute from config file\n",
    "        self.model._included_variables = {}\n",
    "        # keep varaibles from excluded_variable attribute from config file\n",
    "        self.model._excluded_variables = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            self.model._included_variables[action_name]={}\n",
    "            self.model._excluded_variables[action_name]={}\n",
    "\n",
    "            if True in action_config and action_config[True][\"variables\"]:\n",
    "                get_vars = action_config[True][\"variables\"]\n",
    "                bind(self.model, get_vars)\n",
    "                self.model._included_variables[action_name][True] = getattr(self.model, get_vars.__name__)()\n",
    "            if False in action_config and action_config[False][\"variables\"]:\n",
    "                    get_vars = action_config[False][\"variables\"]\n",
    "                    bind(self.model, get_vars)\n",
    "                    self.model._included_variables[action_name][False] = getattr(self.model, get_vars.__name__)()\n",
    "                    \n",
    "            if True in action_config and action_config[True][\"excluded_variables\"]:\n",
    "                get_vars = action_config[True][\"excluded_variables\"]\n",
    "                bind(self.model, get_vars)\n",
    "                self.model._excluded_variables[action_name][True] = getattr(self.model, get_vars.__name__)()            \n",
    "            if False in action_config and action_config[False][\"excluded_variables\"]:\n",
    "                get_vars = action_config[False][\"excluded_variables\"]\n",
    "                bind(self.model, get_vars)\n",
    "                self.model._excluded_variables[action_name][False] = getattr(self.model, get_vars.__name__)()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"-------------------Variables Evaluated for 'variable'/'excluded_varaibles'-------------------\\n\")\n",
    "            print(\"-------------------model._included_variables-------------------\")\n",
    "            print(self.model._included_variables)\n",
    "            print(\"\\n-------------------model._excluded_variables-------------------\")\n",
    "            print(self.model._excluded_variables)\n",
    "\n",
    "    def _bind_reinitialization(self, reinit_config: Dict[str, Any]) -> None:\n",
    "        \n",
    "        def validate_reinit_config(reinit_config: Dict[str, Any]) -> None:\n",
    "            stage, when = reinit_config[\"stage\"], reinit_config[\"when\"]\n",
    "            if stage == \"train\" and when in {\"begin\", \"end\"}:\n",
    "                raise KeyError(f\"You cannot run reinitailizaiton: on_train_{when}. It has no meaning to do so.\")\n",
    "            if (stage == \"batch\" and when == \"end\") or (stage == \"epoch\" and when == \"begin\"):\n",
    "                Warning(f\"reinitalization step is called from the body of internally implemented callback method: on_{stage}_{when}\")\n",
    "        \n",
    "        validate_reinit_config(reinit_config)\n",
    "\n",
    "        bind(self, self.reinit_config[\"cond\"], \"reinit_cond\")\n",
    "\n",
    "        lscope = locals()\n",
    "        stage, when = reinit_config[\"stage\"], reinit_config[\"when\"]\n",
    "\n",
    "        if reinit_config[\"reinit_fn\"]:\n",
    "            Warning(\"binding reinitalization method from reinit_config to the model instance.\")\n",
    "            bind(self.model, reinit_config[\"reinit_fn\"], \"reinitialize\")\n",
    "        else:\n",
    "            if not callable(getattr(self.model.__class__, \"reinitialize\", None)):\n",
    "                raise ReferenceError(\"model has no self.reinitialize() method\")\n",
    "            Warning(\"reinit_fn from reinit_config is none: using self.reinitialize() from the model instance.\")\n",
    "\n",
    "        function_body = f\"\"\"\n",
    "def on_{stage}_{when}(self, {'epoch' if 'epoch' in stage else \"batch\"}, logs=None):\n",
    "    if self.reinit_cond():\n",
    "        tf.print('----Reinitializatoin----')\n",
    "        self.reinit_history.append(self.reinit_current_history)\n",
    "        self.reinit_current_history = []\n",
    "        self.model.reinitialize()\n",
    "        \"\"\"\n",
    "\n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self, lscope[f\"on_{stage}_{when}\"])\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\n-------------------Reinitalization-------------------\")\n",
    "            print(function_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Variables Evaluated for 'variable'/'excluded_varaibles'-------------------\n",
      "\n",
      "-------------------model._included_variables-------------------\n",
      "{'delay': DictWrapper({}), 'gate': DictWrapper({False: ListWrapper([<tf.Variable 'dense_47/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[-0.76974505],\n",
      "       [ 1.3001617 ]], dtype=float32)>])})}\n",
      "\n",
      "-------------------model._excluded_variables-------------------\n",
      "{'delay': DictWrapper({}), 'gate': DictWrapper({True: ListWrapper([<tf.Variable 'dense_47/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[-0.76974505],\n",
      "       [ 1.3001617 ]], dtype=float32)>])})}\n",
      "\n",
      "-------------------MASTER STEP-------------------\n",
      "\n",
      "@tf.function\n",
      "def train_step(self, data):\n",
      "    loss = {'loss_delay' : tf.cond(self.control_variables['delay'], lambda: self.delay_on(data), lambda: self.delay_off(data)),'loss_gate' : tf.cond(self.control_variables['gate'], lambda: self.gate_on(data), lambda: self.gate_off(data))}\n",
      "    metrics = {m.name : m.result() for m in self.metrics}\n",
      "    if \"loss\" not in metrics and self.loss in self.metrics_names:\n",
      "        metrics[\"loss\"] = metrics.pop(self.loss)\n",
      "    control_states = {\n",
      "            control_name: tf.cond(\n",
      "                control_value,\n",
      "                lambda: tf.constant(True),\n",
      "                lambda: tf.constant(False),\n",
      "            )\n",
      "            for control_name, control_value in self.control_variables.items()\n",
      "    }\n",
      "    \n",
      "    return {**loss, **metrics, **control_states}\n",
      "\n",
      "-------------------SLAVE STEPS-------------------\n",
      "\n",
      "-------------------delay_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def delay_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "        \n",
      "        logits = self(x, training=True)\n",
      "        loss_value =  loss(y, logits)\n",
      "        \n",
      "        loss_value += tf.math.reduce_sum(self.losses)\n",
      "        \n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------delay_off-------------------\n",
      "\n",
      "@tf.function\n",
      "def delay_off(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "        \n",
      "        logits = self(x, training=True)\n",
      "        loss_value =  loss(y, logits)\n",
      "        \n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      "        \n",
      "        for g in self._excluded_variables['gate'][True]:\n",
      "            tape.watch(g)\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = tf.constant(0, dtype=tf.float32) * loss(y, logits)\n",
      "        \n",
      "        loss_value += tf.math.reduce_sum(self.losses)\n",
      "        \n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.1, 0.1) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_off-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_off(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      "        \n",
      "        for g in self._included_variables['gate'][False]:\n",
      "            tape.watch(g)\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value =  loss(y, logits)\n",
      "        \n",
      "        loss_value += tf.math.reduce_sum(self.losses)\n",
      "        \n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.1, 0.1) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "in user code:\n\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"<string>\", line 4, in train_step\n        \n    File \"<string>\", line 4, in <lambda>\n        \n    File \"<string>\", line 4, in delay_on\n        \n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X10sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m lcc \u001b[39m=\u001b[39m LoopControlerCallback(config, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X10sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# start training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X10sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(data, epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X10sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[lcc])\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1269\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1269\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1270\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1271\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: in user code:\n\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"<string>\", line 4, in train_step\n        \n    File \"<string>\", line 4, in <lambda>\n        \n    File \"<string>\", line 4, in delay_on\n        \n\n    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(INPUT_SIZE),\n",
    "    tf.keras.layers.Dense(OUTPUT_SIZE),\n",
    "    tf.keras.layers.Dense(OUTPUT_SIZE)\n",
    "    ])\n",
    "# compile model\n",
    "model.compile(optimizer=\n",
    "    tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "def reinit_cond(self):\n",
    "    split_index = len(self.reinit_config) // 2\n",
    "    return len(self.reinit_config) > 10000  and tf.math.reduce_mean(self.reinit_config[split_index:]) > 0.7\n",
    "\n",
    "def reinitalize_dummy(self):\n",
    "    tf.print(\"\\nFUCK\\n\")\n",
    "\n",
    "reinit_config = {\n",
    "    \"cond\": reinit_cond,\n",
    "    \"reinit_fn\": reinitalize_dummy,\n",
    "    \"when\": \"end\",\n",
    "    \"stage\": \"epoch\"\n",
    "}\n",
    "\n",
    "\n",
    "def gate_config(self):\n",
    "    return self.batches % 2 == 1\n",
    "\n",
    "def delay_config(self):\n",
    "    return self.epochs > 5\n",
    "\n",
    "def dummy_vars(self):\n",
    "    if 2 < 3:\n",
    "        return [self.variables[0]]\n",
    "    return [self.variables[1]]\n",
    "\n",
    "    \n",
    "config = {\n",
    "    \"delay\": {\n",
    "        \"cond\": delay_config,\n",
    "        True: {\n",
    "            \"regularize\": True\n",
    "        },\n",
    "        False: {},\n",
    "    },\n",
    "    \"gate\": {\n",
    "        \"cond\": gate_config,\n",
    "        True: {\n",
    "            \"loss\": False,\n",
    "            \"clipping\": (-0.1, 0.1),\n",
    "            \"excluded_variables\": dummy_vars,\n",
    "            \"regularize\": True\n",
    "        },\n",
    "        False: {\n",
    "            \"clipping\": (-0.1, 0.1),\n",
    "            \"variables\": dummy_vars,\n",
    "            \"regularize\": True\n",
    "\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "lcc = LoopControlerCallback(config, verbose=1)\n",
    "# start training\n",
    "history = model.fit(data, epochs = 10, verbose=1,\n",
    "    callbacks=[lcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.utils import types\n",
    "from typeguard import typechecked\n",
    "\n",
    "\n",
    "class NALURegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, reg_coef=0.1):\n",
    "        self.reg_coef = reg_coef\n",
    "\n",
    "    def __call__(self, var: List[tf.Variable]) -> tf.Tensor:\n",
    "        return self.reg_coef * tf.add_n(\n",
    "            [\n",
    "                tf.reduce_mean(tf.math.maximum(tf.math.minimum(-v, v) + 20, 0))\n",
    "                for v in var\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"reg_coef\": float(self.reg_coef)}\n",
    "\n",
    "class NALU(tf.keras.layers.Layer):\n",
    "\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        units: int,\n",
    "        regularizer: types.Regularizer = NALURegularizer(reg_coef=0.05),\n",
    "        clipping: float = 20,\n",
    "        w_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=1.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        m_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=-1.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        g_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=0.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(NALU, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.reg_fn = regularizer\n",
    "        self.clipping = clipping\n",
    "\n",
    "        self.w_initializer = w_initializer\n",
    "        self.m_initializer = m_initializer\n",
    "        self.g_initializer = g_initializer\n",
    "\n",
    "        self.gate_as_vector = True\n",
    "        self.force_operation = None\n",
    "        self.weights_separation = True\n",
    "        self.input_gate_dependance = False\n",
    "        self.initializer = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # action variables\n",
    "        self.w_hat = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.w_initializer,\n",
    "            trainable = True,\n",
    "            name = \"w\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        self.m_hat = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.m_initializer,\n",
    "            trainable = True,\n",
    "            name = \"m\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        self.w_hat_prime = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.w_initializer,\n",
    "            trainable = True,\n",
    "            name = \"w_prime\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        self.m_hat_prime = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.m_initializer,\n",
    "            trainable = True,\n",
    "            name = \"m_prime\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        # gating varaible\n",
    "        self.g = self.add_weight (\n",
    "            shape = (self.units, ),\n",
    "            initializer = self.g_initializer,\n",
    "            trainable = True,\n",
    "            name = \"g\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def get_reg_loss(self):\n",
    "        var_list = [self.w_hat, self.m_hat, self.g]\n",
    "        if self.weights_separation:\n",
    "            var_list += [self.w_hat_prime, self.m_hat_prime]\n",
    "        return self.reg_fn(var_list)\n",
    "\n",
    "    def call(self, input):\n",
    "        eps = 1e-7\n",
    "        w1 = tf.math.tanh(self.w_hat) * tf.math.sigmoid(self.m_hat)\n",
    "        w2 = tf.math.tanh(self.w_hat_prime) * tf.math.sigmoid(self.m_hat_prime)\n",
    "        a1 = tf.matmul(input, w1)\n",
    "\n",
    "        m1 = tf.math.exp(tf.minimum(tf.matmul(tf.math.log(tf.maximum(tf.math.abs(input), eps)),w2), self.clipping))\n",
    "        \n",
    "        # sign\n",
    "        w1s = tf.math.abs(tf.reshape(w2, [-1]))\n",
    "        xs = tf.concat([input] * w1.shape[1], axis=1)\n",
    "        xs = tf.reshape(xs, shape=[-1, w1.shape[0] * w1.shape[1]])\n",
    "        sgn = tf.sign(xs) * w1s + (1 - w1s)\n",
    "        sgn = tf.reshape(sgn, shape=[-1, w1.shape[1], w1.shape[0]])\n",
    "        ms = tf.math.reduce_prod(sgn, axis=2)\n",
    "        \n",
    "        self.add_loss(lambda: self.get_reg_loss())\n",
    "        g1 = tf.math.sigmoid(self.g)\n",
    "        return g1 * a1 + (1 - g1) * m1 * tf.clip_by_value(ms, -1, 1)\n",
    "\n",
    "    def reinitialize(self):\n",
    "        self.g.assign(self.g_initializer(self.g.shape))\n",
    "        self.w_hat.assign(self.w_initializer(self.w_hat.shape))\n",
    "        self.m_hat.assign(self.m_initializer(self.m_hat.shape))\n",
    "        self.w_hat_prime.assign(self.w_initializer(self.w_hat_prime.shape))\n",
    "        self.m_hat_prime.assign(self.m_initializer(self.m_hat_prime.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t 10\n",
      "\t\tmean(s)\t 12.5\n",
      "\t\tdata <\t 15\n",
      "\t\tstd \t 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "from scipy.stats import truncnorm\n",
    "import ast\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-o\", \"--output\", dest=\"output\", default=\"naly_syn_simple_arith\")\n",
    "parser.add_argument(\"-d\", \"--dist\", dest=\"dist\", default=\"normal\", help=\"Prob.Dist\")\n",
    "parser.add_argument(\n",
    "    \"-p\", \"--params\", dest=\"params\", default=\"(-3,3)\", type=ast.literal_eval\n",
    ")\n",
    "parser.add_argument(\"-e\", \"--ext\", dest=\"ext\", default=\"(10,15)\", type=ast.literal_eval)\n",
    "\n",
    "parser.add_argument(\"-n\", \"--nalu\", dest=\"nalu\", default=\"nalui1\")\n",
    "parser.add_argument(\"-se\", \"--seed\", dest=\"seed\", default=42, type=int)\n",
    "parser.add_argument(\"-op\", \"--operation\", dest=\"op\", default=\"MUL\")\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "def sample(dist, params, numDim=3, numDP=64000):\n",
    "    data = np.zeros(shape=(numDP, numDim))\n",
    "    if dist == \"normal\":\n",
    "        intmean = (params[0] + params[1]) / 2\n",
    "        intstd = (params[1] - params[0]) / 6\n",
    "        print(\n",
    "            \"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tmean(s)\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\tstd \\t %s\"\n",
    "            % (dist, params[0], intmean, params[1], intstd)\n",
    "        )\n",
    "        mi, ma = (params[0] - intmean) / intstd, (params[1] - intmean) / intstd\n",
    "        data = np.reshape(\n",
    "            truncnorm.rvs(mi, ma, intmean, intstd, size=numDim * numDP), data.shape\n",
    "        )\n",
    "\n",
    "    elif dist == \"uniform\":\n",
    "        print(\n",
    "            \"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\t\"\n",
    "            % (dist, params[0], params[1])\n",
    "        )\n",
    "        data = np.reshape(\n",
    "            np.random.uniform(params[0], params[1], size=numDim * numDP), data.shape\n",
    "        )\n",
    "    elif dist == \"exponential\":\n",
    "        data = np.random.exponential(params, size=(numDP, numDim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown distribution\")\n",
    "    data = np.reshape(data, [-1])  # reshape to mix both distributions per instance!\n",
    "    np.random.shuffle(data)\n",
    "    data = np.reshape(data, (numDP, numDim))\n",
    "    return data\n",
    "\n",
    "\n",
    "def operation(op, a, b):\n",
    "    if op.lower() == \"mul\":\n",
    "        return a * b\n",
    "    if op.lower() == \"add\":\n",
    "        return a + b\n",
    "    if op.lower() == \"sub\":\n",
    "        return a - b\n",
    "    if op.lower() == \"div\":\n",
    "        return a / b\n",
    "\n",
    "input_dim = 7\n",
    "output_dim = 1\n",
    "\n",
    "def data_comb(data):\n",
    "    return (data[:, 0] - data[:, 1]) *  (data[:, 2] - data[:, 3]) +  (data[:, 4] * data[:, 5]) \n",
    "\n",
    "data = sample(args.dist, args.params, input_dim)\n",
    "lbls =  data_comb(data)\n",
    "lbls = np.reshape(lbls, newshape=(-1, 1))\n",
    "\n",
    "int_data = sample(args.dist, args.params, input_dim)\n",
    "int_lbls =  data_comb(int_data)\n",
    "int_lbls = np.reshape(int_lbls, newshape=(-1, 1))\n",
    "\n",
    "\n",
    "ext_data = sample(args.dist, args.ext, input_dim)\n",
    "ext_lbls =  data_comb(ext_data)\n",
    "ext_lbls = np.reshape(ext_lbls, newshape=(-1, 1))\n",
    "\n",
    "\n",
    "data_dp = tf.data.Dataset.from_tensor_slices((data, lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "int_data_dp = tf.data.Dataset.from_tensor_slices((int_data, int_lbls)).prefetch(tf.data.AUTOTUNE)\n",
    "ext_data_dp = tf.data.Dataset.from_tensor_slices((ext_data, ext_lbls)).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Variables Evaluated for 'variable'/'excluded_varaibles'-------------------\n",
      "\n",
      "-------------------model._included_variables-------------------\n",
      "{'delay': DictWrapper({}), 'gate': DictWrapper({False: ListWrapper([<tf.Variable 'nalu_48/g:0' shape=(3,) dtype=float32, numpy=array([0.09648914, 0.0719344 , 0.02736734], dtype=float32)>, <tf.Variable 'nalu_49/g:0' shape=(2,) dtype=float32, numpy=array([-0.00828246,  0.00123813], dtype=float32)>, <tf.Variable 'nalu_50/g:0' shape=(1,) dtype=float32, numpy=array([-0.06378405], dtype=float32)>])})}\n",
      "\n",
      "-------------------model._excluded_variables-------------------\n",
      "{'delay': DictWrapper({}), 'gate': DictWrapper({True: ListWrapper([<tf.Variable 'nalu_48/g:0' shape=(3,) dtype=float32, numpy=array([0.09648914, 0.0719344 , 0.02736734], dtype=float32)>, <tf.Variable 'nalu_49/g:0' shape=(2,) dtype=float32, numpy=array([-0.00828246,  0.00123813], dtype=float32)>, <tf.Variable 'nalu_50/g:0' shape=(1,) dtype=float32, numpy=array([-0.06378405], dtype=float32)>])})}\n",
      "\n",
      "-------------------MASTER STEP-------------------\n",
      "\n",
      "@tf.function\n",
      "def train_step(self, data):\n",
      "    loss = {'loss_delay' : tf.cond(self.control_variables['delay'], lambda: self.delay_on(data), lambda: 0.0),'loss_gate' : tf.cond(self.control_variables['gate'], lambda: self.gate_on(data), lambda: self.gate_off(data))}\n",
      "    metrics = {m.name : m.result() for m in self.metrics}\n",
      "    if \"loss\" not in metrics and self.loss in self.metrics_names:\n",
      "        metrics[\"loss\"] = metrics.pop(self.loss)\n",
      "    control_states = {\n",
      "            control_name: tf.cond(\n",
      "                control_value,\n",
      "                lambda: tf.constant(True),\n",
      "                lambda: tf.constant(False),\n",
      "            )\n",
      "            for control_name, control_value in self.control_variables.items()\n",
      "    }\n",
      "    \n",
      "    return {**loss, **metrics, **control_states}\n",
      "\n",
      "-------------------SLAVE STEPS-------------------\n",
      "\n",
      "-------------------delay_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def delay_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "        \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = tf.constant(0, dtype=tf.float32) * loss(y, logits)\n",
      "        \n",
      "        loss_value += tf.math.reduce_sum(self.losses)\n",
      "        \n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      "        \n",
      "        for g in self._excluded_variables['gate'][True]:\n",
      "            tape.watch(g)\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value =  loss(y, logits)\n",
      "        \n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.1, 0.1) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_off-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_off(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      "        \n",
      "        for g in self._included_variables['gate'][False]:\n",
      "            tape.watch(g)\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value =  loss(y, logits)\n",
      "        \n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.1, 0.1) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "\n",
      "-------------------Reinitalization-------------------\n",
      "\n",
      "def on_epoch_end(self, epoch, logs=None):\n",
      "    if self.reinit_cond():\n",
      "        tf.print('----Reinitializatoin----')\n",
      "        self.reinit_history.append(self.reinit_current_history)\n",
      "        self.reinit_current_history = []\n",
      "        self.model.reinitialize()\n",
      "        \n",
      "Epoch 1/80\n",
      "1000/1000 [==============================] - 10s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 4.7207 - loss: 0.0000e+00 - mse: 4.7219 - mae: 1.5119 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 2/80\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 4.7093 - loss: 0.0000e+00 - mse: 4.7105 - mae: 1.5084 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 3/80\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 4.7074 - loss: 0.0000e+00 - mse: 4.7086 - mae: 1.5078 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 4/80\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 4.7067 - loss: 0.0000e+00 - mse: 4.7079 - mae: 1.5077 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 5/80\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 4.7065 - loss: 0.0000e+00 - mse: 4.7077 - mae: 1.5076 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 6/80\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 4.7065 - loss: 0.0000e+00 - mse: 4.7077 - mae: 1.5076 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 7/80\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 4.7065 - loss: 0.0000e+00 - mse: 4.7077 - mae: 1.5076 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 8/80\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 4.7065 - loss: 0.0000e+00 - mse: 4.7076 - mae: 1.5076 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 9/80\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 4.7064 - loss: 0.0000e+00 - mse: 4.7076 - mae: 1.5076 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 10/80\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 4.7064 - loss: 0.0000e+00 - mse: 4.7076 - mae: 1.5076 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 11/80\n",
      "1000/1000 [==============================] - ETA: 0s - loss_delay: 27.7416 - loss_gate: 4.7074 - loss: 4.7075 - mse: 4.7075 - mae: 1.5076 - delay: 1.0000 - gate: 0.1000----Reinitializatoin----\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss_delay: 27.7300 - loss_gate: 4.7062 - loss: 4.7075 - mse: 4.7075 - mae: 1.5076 - delay: 1.0000 - gate: 0.1009\n",
      "Epoch 12/80\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss_delay: 31.7089 - loss_gate: 5.4333 - loss: 5.4318 - mse: 5.4326 - mae: 1.7123 - delay: 1.0000 - gate: 0.1009\n",
      "Epoch 13/80\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss_delay: 8.6061 - loss_gate: 5.7068 - loss: 5.7072 - mse: 5.7072 - mae: 1.7851 - delay: 1.0000 - gate: 0.1009\n",
      "Epoch 14/80\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss_delay: 0.0000e+00 - loss_gate: 5.7069 - loss: 5.7073 - mse: 5.7073 - mae: 1.7851 - delay: 1.0000 - gate: 0.1009\n",
      "Epoch 15/80\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss_delay: 0.0000e+00 - loss_gate: 5.7069 - loss: 5.7073 - mse: 5.7073 - mae: 1.7851 - delay: 1.0000 - gate: 0.1009\n",
      "Epoch 16/80\n",
      " 803/1000 [=======================>......] - ETA: 1s - loss_delay: 0.0000e+00 - loss_gate: 5.6791 - loss: 5.6791 - mse: 5.6791 - mae: 1.7804 - delay: 1.0000 - gate: 0.0996"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X16sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m lcc \u001b[39m=\u001b[39m LoopControlerCallback(config, reinit_config, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X16sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# start training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X16sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(data_dp, epochs \u001b[39m=\u001b[39;49m \u001b[39m80\u001b[39;49m, verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X16sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[lcc])\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    917\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    920\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    921\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_dim),\n",
    "    NALU(3, clipping = 20),\n",
    "    NALU(2, clipping = 20),\n",
    "    NALU(1, clipping = 20)\n",
    "    ])\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=\n",
    "    tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mse\", \"mae\"])\n",
    "\n",
    "def gate_open(self):\n",
    "    return self.batches % 10 > 8\n",
    "\n",
    "def delay_reg(self):\n",
    "    return self.epochs > 10\n",
    "    \n",
    "def get_gates_variables(self) -> List[tf.Variable]:\n",
    "    return [l.g for l in self.layers if isinstance(l, NALU)]\n",
    "    \n",
    "config = {\n",
    "    \"delay\": {\n",
    "        \"cond\": delay_reg,\n",
    "        True: {\n",
    "            \"loss\": False,\n",
    "            \"regularize\": True\n",
    "        }\n",
    "    },\n",
    "    \"gate\": {\n",
    "        \"cond\": gate_open,\n",
    "        True: {\n",
    "            \"clipping\": (-0.1, 0.1),\n",
    "            \"excluded_variables\": get_gates_variables,\n",
    "        },\n",
    "        False: {\n",
    "            \"clipping\": (-0.1, 0.1),\n",
    "            \"variables\": get_gates_variables\n",
    "        }\n",
    "    },\n",
    "   \n",
    "}\n",
    "\n",
    "\n",
    "def reinit_cond(self):\n",
    "    reinit_current_loss_history = [x[\"loss\"] for x in self.reinit_current_history]\n",
    "    split_index = len(reinit_current_loss_history) // 2\n",
    "    return (len(reinit_current_loss_history) > 10000) and (tf.math.reduce_mean(reinit_current_loss_history[split_index:]) > 0.7)\n",
    "\n",
    "def reinitialize_fn(self):\n",
    "    for l in self.layers:\n",
    "        if isinstance(l, NALU):\n",
    "            l.reinitialize()\n",
    "\n",
    "reinit_config = {\n",
    "    \"cond\": reinit_cond,\n",
    "    \"reinit_fn\": reinitialize_fn,\n",
    "    \"when\": \"end\",\n",
    "    \"stage\": \"epoch\"\n",
    "}\n",
    "\n",
    "lcc = LoopControlerCallback(config, reinit_config, verbose=1)\n",
    "# start training\n",
    "history = model.fit(data_dp, epochs = 80, verbose = 1,\n",
    "    callbacks=[lcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b4ff1917ad7fe489d902c3041fcac2959f3e2431a3acc02677f15f306757aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
