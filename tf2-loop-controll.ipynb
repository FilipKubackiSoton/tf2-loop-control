{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow2 training loop control using default *tf.fit(...)* function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Descripton\n",
    "\n",
    "Up to now custom training loop in Tensorflow2 requires writing two lops:\n",
    "1. loop iterating through epochs \n",
    "2. loop iterating through batches \n",
    "\n",
    "Then all castom training precudere will have to be implemented in these double-loop block of code. It's neither elegant nor robust due to the missing advanced features of *tf.fit(...)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind(instance, func, as_name=None):\n",
    "    \"\"\"\n",
    "    Bind the function *func* to *instance*, with either provided name *as_name*\n",
    "    or the existing name of *func*. The provided *func* should accept the \n",
    "    instance as the first argument, i.e. \"self\".\n",
    "    \"\"\"\n",
    "    if as_name is None:\n",
    "        as_name = func.__name__\n",
    "    bound_method = func.__get__(instance, instance.__class__)\n",
    "    setattr(instance, as_name, bound_method)\n",
    "    return bound_method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gate_on(self, data):\n",
    "    x, y = data\n",
    "        \n",
    "    print(\"----------!!!!!!----------\")\n",
    "    print(self.gate_on_vars)\n",
    "    if not self.gate_on_vars:\n",
    "        self.gate_on_vars=self.get_gate_on_vars()\n",
    "    print(self.gate_on_vars())\n",
    "    \n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        for g in self.gate_on_vars:\n",
    "            tape.watch(g)\n",
    "            \n",
    "        logits = self(x, training=True)\n",
    "        loss_value = loss(y, logits)\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.2, 0.3) for g in grads], tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "    return loss_value\n",
    "\n",
    "class LoopControlerCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, config: int, default_in_branch: Dict[str, Any] = None, verbose: bool = True, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super(LoopControlerCallback, self).__init__(*args, **kwargs)\n",
    "        self.default_in_branch: Dict[str, Any] = default_in_branch if default_in_branch else {\n",
    "                \"loss\": None, \n",
    "                \"clipping\": None,\n",
    "                \"variables\": None,\n",
    "                \"exclud_var\": None,\n",
    "            }\n",
    "        self.config: Dict[str, Any] = config\n",
    "        self.verbose: bool = verbose\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"Function called directely before training. It is executed on each call of model.fit with this callback.\n",
    "            Inside the scope of this funciton we can access model on which this callback works: self.model.\n",
    "\n",
    "        Args:\n",
    "            logs (_type_, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        # meta attributes for building conditions\n",
    "        self.epochs: int = 0\n",
    "        self.batches: int = 0\n",
    "        self.batches_in_epoch: int = 0\n",
    "        self.last_loss: int = 0.0\n",
    "        self.history: List[Any] = []\n",
    "        \n",
    "        # if loss in default branch is None, then use compiled loss\n",
    "        if not self.default_in_branch[\"loss\"]:\n",
    "            self.default_in_branch[\"loss\"] = self.model.compiled_loss\n",
    "\n",
    "        self.model.cv_names = []\n",
    "        self.c_conds = {}\n",
    "\n",
    "        # extend config with validation step\n",
    "        self.config = self._extend_config(self.config)\n",
    "        # initiate control variables\n",
    "        self._init_cv(self.config)\n",
    "        # bind control variables to model\n",
    "        self._bind_cv(self.config)\n",
    "        # bind master train_step to model\n",
    "        self._bind_master_step(self.config)\n",
    "        # bind placeholders for list of variables for slave steps\n",
    "        self._bind_slaves_steps_dif_variables(self.config)\n",
    "        # bind slave train_steps to model\n",
    "        self._bin_slaves_steps()\n",
    "\n",
    "        # test buingin vars\n",
    "        setattr(self.model, \"gate_on_vars\", None)\n",
    "        bind(self.model, dummy_vars, \"get_gate_on_vars\")\n",
    "        bind(self.model, gate_on)\n",
    "\n",
    "    def _extend_config(self, config: Dict[str, Any]) -> None:\n",
    "        \"\"\"Extend and validate config file. Fill missing fields based on the default_in_branch.\n",
    "\n",
    "        Args:\n",
    "            config (Dict[str, Any]): Configuration to control model training\n",
    "        \"\"\"\n",
    "\n",
    "        def validate_action_config(action_name: str, action_config: Dict[str, Any]) -> None:\n",
    "            \"\"\"Validate model training configuration.\n",
    "\n",
    "            Args:\n",
    "                action_name (str): name of the action slave train step\n",
    "                action_config (Dict[str, Any]): configuration of the action slave train step\n",
    "\n",
    "            Raises:\n",
    "                ValueError: Missing controlable cond\n",
    "                ValueError: Missing branch configuration for true/false after cond\n",
    "            \"\"\"\n",
    "\n",
    "            Warning(f\"------Validating Configuration for {action_name}------\")\n",
    "            if action_config == {}:\n",
    "                Warning(\n",
    "                    f\"{action_name} has empty body. Condition and False or True branch must be implemented.\\n It's ignored in furhter computations\"\n",
    "                )\n",
    "            if (True not in action_config) and (False not in action_config):\n",
    "                raise ValueError(\n",
    "                    f\"{action_name} has no False or True branch implemented\"\n",
    "                )\n",
    "            if \"cond\" not in action_config:\n",
    "                raise ValueError(f\"{action_name} has no condition implemented.\")\n",
    "\n",
    "        pc = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            validate_action_config(action_name, action_config)\n",
    "            pc[action_name] = {\"cond\": action_config[\"cond\"]}\n",
    "            if True in action_config:\n",
    "                pc[action_name][True] = {**self.default_in_branch, **action_config[True]}\n",
    "            if False in action_config:\n",
    "                pc[action_name][False] = {**self.default_in_branch, **action_config[False]}\n",
    "\n",
    "        return pc\n",
    "\n",
    "    def _get_cv_name(self, action_name: str) -> str:\n",
    "        \"\"\"Get control variable from action name\n",
    "\n",
    "        Args:\n",
    "            action_name (str): name of the action slave train step\n",
    "\n",
    "        Returns:\n",
    "            str: converted name of the action slave train step\n",
    "        \"\"\"\n",
    "        return f\"{action_name}_cv\"\n",
    "\n",
    "    def _get_cc_names(self, condition_name: str) -> Tuple[str, str]:\n",
    "        return (f\"{condition_name}_off\", f\"{condition_name}_on\")\n",
    "\n",
    "    def _init_cv(self, config) -> None:\n",
    "        for cv_name in config.keys():\n",
    "            setattr(self.model, cv_name, tf.Variable(False, trainable=False))\n",
    "            self.model.cv_names.append(cv_name)\n",
    "\n",
    "    def _bind_cv(self, config) -> None:\n",
    "        for action_name, action_config in config.items():\n",
    "            name = self._get_cv_name(action_name)\n",
    "            bind(self, action_config[\"cond\"], name)\n",
    "            self.c_conds[action_name] = name\n",
    "\n",
    "    def _bind_master_step(self, config) -> None:\n",
    "        lscope = locals()\n",
    "        def _get_losses(config: Dict[str, Any]) -> str:\n",
    "            def _substeps_condition_execution(name: str, conf: Dict[str, Any], on: bool) -> str:\n",
    "                if on:\n",
    "                    return f\"self.{name}_on(data)\" if True in conf else \"0.0\"\n",
    "                else:\n",
    "                    return f\"self.{name}_off(data)\" if False in conf else \"0.0\"\n",
    "\n",
    "            return \"{\" + \",\".join([\n",
    "                        f\"'loss_{an}' : tf.cond(self.{an}, lambda: {_substeps_condition_execution(an, ac, True)}, lambda: {_substeps_condition_execution(an, ac, False)})\"\n",
    "                        for an, ac in config.items()\n",
    "                    ]) + \"}\"\n",
    "\n",
    "        lscope = locals()\n",
    "        function_body = \"\"\"\n",
    "@tf.function\n",
    "def train_step(self, data):\n",
    "    loss = {losses_config}\n",
    "    metrics = {{m.name : m.result() for m in self.metrics}}\n",
    "    \n",
    "    control_states = {{\n",
    "            c_name: tf.cond(\n",
    "                getattr(self, c_name),\n",
    "                lambda: tf.constant(True),\n",
    "                lambda: tf.constant(False),\n",
    "            )\n",
    "            for c_name in self.cv_names\n",
    "    }}\n",
    "    \n",
    "    return {{**loss, **metrics, **control_states}}\n",
    "\"\"\".format(\n",
    "            **{\"losses_config\": _get_losses(config)}\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"-------------------MASTER STEP-------------------\")\n",
    "            print(function_body)\n",
    "        \n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[\"train_step\"])\n",
    "\n",
    "    def _get_name_for_sleve_step_variables(self, name, on = None):\n",
    "        if on == None:\n",
    "            return f\"{name}_vars\"\n",
    "        return f\"{name}_{'on' if on else 'off'}_vars\"\n",
    "\n",
    "    def _bind_slaves_steps_dif_variables(self, config):\n",
    "        for action_name, action_config in config.items():\n",
    "            if action_config[True] and \"variables\" in action_config[True] and action_config[True][\"variables\"]:\n",
    "                # we have to bind None because variables before trainign are not initialised\n",
    "                setattr(self.model, self._get_name_for_sleve_step_variables(action_name, True), action_config[True][\"variables\"])\n",
    "            \n",
    "            if False in action_config and \"variables\" in action_config[False] and action_config[False][\"variables\"]:\n",
    "                # we have to bind None because variables before trainign are not initialised\n",
    "                setattr(self.model, self._get_name_for_sleve_step_variables(action_name, False), action_config[False][\"variables\"])\n",
    "\n",
    "    def _bin_slaves_steps(self) -> None:\n",
    "        if self.verbose:\n",
    "            print(\"-------------------SLAVE STEPS-------------------\\n\")\n",
    "        \n",
    "        for action_name, action_config in self.config.items():\n",
    "            off_step_name, on_step_name = self._get_cc_names(action_name)\n",
    "            if True in action_config:\n",
    "                self._bind_slave_step(on_step_name, action_config[True], True)\n",
    "            if False in action_config:\n",
    "                self._bind_slave_step(off_step_name, action_config[False], False)\n",
    "\n",
    "    def _bind_slave_step(self, fn_name: str, fn_config: Dict[str, Any], true_branch: bool) -> None:\n",
    "        \n",
    "        lscope = {\n",
    "            **locals(), \n",
    "            **fn_config\n",
    "            }\n",
    "        \n",
    "        print(lscope)\n",
    "\n",
    "        function_body = f\"\"\"\n",
    "@tf.function\n",
    "def {fn_name}(self, data):\n",
    "    x, y = data\n",
    "        \"\"\"\n",
    "\n",
    "        if fn_config[\"variables\"]:\n",
    "            vars_name = self._get_name_for_sleve_step_variables(fn_name)\n",
    "            get_vars_fname=f\"get_{fn_name}_vars\"\n",
    "            setattr(self.model, vars_name, None)\n",
    "            bind(self.model, lscope[\"variables\"], get_vars_fname)\n",
    "            lscope[get_vars_fname] = lscope[\"variables\"]\n",
    "            function_body += f\"\"\"\n",
    "    print(\"----------!!!----------\")\n",
    "    print(self.{vars_name})\n",
    "    if not self.{vars_name}:\n",
    "        self.{vars_name}=self.{get_vars_fname}()\n",
    "    print(self.{vars_name}())\n",
    "    \n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        for g in self.{vars_name}:\n",
    "            tape.watch(g)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            function_body += \"\"\"\n",
    "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
    "            \"\"\"\n",
    "\n",
    "        function_body += \"\"\"\n",
    "        logits = self(x, training=True)\n",
    "        loss_value = loss(y, logits)\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip({clipping_grads}, tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "    return loss_value\n",
    "\"\"\".format(**{\n",
    "    \"clipping_grads\": \"[tf.clip_by_value(g, {clip_low}, {clip_high}) for g in grads]\".format(**{\n",
    "        \"clip_low\": fn_config[\"clipping\"][0], \"clip_high\": fn_config[\"clipping\"][1]\n",
    "        }) if fn_config[\"clipping\"] else \"grads\",\n",
    "})\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"-------------------{fn_name}-------------------\")\n",
    "            print(fn_config[\"variables\"])\n",
    "            print(function_body)\n",
    "        \n",
    "        if not fn_config[\"variables\"]:\n",
    "            exec(function_body, {**globals(), **lscope}, lscope)\n",
    "            bind(self.model, lscope[fn_name])\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs) -> None:\n",
    "        self.epochs += 1\n",
    "        \"\"\"Control gating variable from the level of callback which can work on epoch/batch level.\"\"\"\n",
    "        # tf.variable.assign is different than tf.variable = <sth>. The second option is compiled to static\n",
    "        # value in TF graph of computation as the result of @tf.function decorators in LoopControlableModel\n",
    "        for control_name, control_function_name in self.c_conds.items():\n",
    "            getattr(self.model, control_name).assign(\n",
    "                getattr(self, control_function_name)()\n",
    "            )\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        print(self.model.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------MASTER STEP-------------------\n",
      "\n",
      "@tf.function\n",
      "def train_step(self, data):\n",
      "    loss = {'loss_gate' : tf.cond(self.gate, lambda: self.gate_on(data), lambda: self.gate_off(data)),'loss_delay' : tf.cond(self.delay, lambda: self.delay_on(data), lambda: 0.0)}\n",
      "    metrics = {m.name : m.result() for m in self.metrics}\n",
      "    \n",
      "    control_states = {\n",
      "            c_name: tf.cond(\n",
      "                getattr(self, c_name),\n",
      "                lambda: tf.constant(True),\n",
      "                lambda: tf.constant(False),\n",
      "            )\n",
      "            for c_name in self.cv_names\n",
      "    }\n",
      "    \n",
      "    return {**loss, **metrics, **control_states}\n",
      "\n",
      "-------------------SLAVE STEPS-------------------\n",
      "\n",
      "{'self': <__main__.LoopControlerCallback object at 0x7ff457953d90>, 'fn_name': 'gate_on', 'fn_config': {'loss': <keras.losses.MeanAbsoluteError object at 0x7ff478638e20>, 'clipping': (-0.2, 0.3), 'variables': None, 'exclud_var': None}, 'true_branch': True, 'loss': <keras.losses.MeanAbsoluteError object at 0x7ff478638e20>, 'clipping': (-0.2, 0.3), 'variables': None, 'exclud_var': None}\n",
      "-------------------gate_on-------------------\n",
      "None\n",
      "\n",
      "@tf.function\n",
      "def gate_on(self, data):\n",
      "    x, y = data\n",
      "        \n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.2, 0.3) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "{'self': <__main__.LoopControlerCallback object at 0x7ff457953d90>, 'fn_name': 'gate_off', 'fn_config': {'loss': <keras.engine.compile_utils.LossesContainer object at 0x7ff4579a1550>, 'clipping': None, 'variables': None, 'exclud_var': None}, 'true_branch': False, 'loss': <keras.engine.compile_utils.LossesContainer object at 0x7ff4579a1550>, 'clipping': None, 'variables': None, 'exclud_var': None}\n",
      "-------------------gate_off-------------------\n",
      "None\n",
      "\n",
      "@tf.function\n",
      "def gate_off(self, data):\n",
      "    x, y = data\n",
      "        \n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "{'self': <__main__.LoopControlerCallback object at 0x7ff457953d90>, 'fn_name': 'delay_on', 'fn_config': {'loss': <keras.losses.MeanSquaredError object at 0x7ff4579a1dc0>, 'clipping': None, 'variables': None, 'exclud_var': None}, 'true_branch': True, 'loss': <keras.losses.MeanSquaredError object at 0x7ff4579a1dc0>, 'clipping': None, 'variables': None, 'exclud_var': None}\n",
      "-------------------delay_on-------------------\n",
      "None\n",
      "\n",
      "@tf.function\n",
      "def delay_on(self, data):\n",
      "    x, y = data\n",
      "        \n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "Epoch 1/10\n",
      "----------!!!!!!----------\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_7363/1499073282.py\", line 8, in gate_on  *\n        self.gate_on_vars=self.get_gate_on_vars()\n    File \"/tmp/ipykernel_7363/3443802096.py\", line 31, in dummy_vars  **\n        return [self.variables[0]]\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 2066, in variables\n        return self.weights\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 2829, in weights\n        return self._dedup_weights(self._undeduplicated_weights)\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 2834, in _undeduplicated_weights\n        self._assert_weights_created()\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/sequential.py\", line 472, in _assert_weights_created\n        super(functional.Functional, self)._assert_weights_created()  # pylint: disable=bad-super-call\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 3027, in _assert_weights_created\n        raise ValueError(f'Weights for model {self.name} have not yet been '\n\n    ValueError: Weights for model sequential_79 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m lcc \u001b[39m=\u001b[39m LoopControlerCallback(config)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#W5sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# start training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(data, epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[lcc])\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filerihwkru0.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filef1868p0w.py:25\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__gate_on\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39melse_body\u001b[39m():\n\u001b[1;32m     24\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mnot_(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mgate_on_vars), if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mself.gate_on_vars\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mgate_on_vars, (), \u001b[39mNone\u001b[39;00m, fscope))\n\u001b[1;32m     27\u001b[0m \u001b[39mwith\u001b[39;00m ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mGradientTape(watch_accessed_variables\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m tape:\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filef1868p0w.py:21\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__gate_on.<locals>.if_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mif_body\u001b[39m():\n\u001b[0;32m---> 21\u001b[0m     ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mgate_on_vars \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mget_gate_on_vars, (), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "\u001b[1;32m/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb Cell 6\u001b[0m in \u001b[0;36mdummy_vars\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdummy_vars\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariables[\u001b[39m0\u001b[39m]]\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_7363/1499073282.py\", line 8, in gate_on  *\n        self.gate_on_vars=self.get_gate_on_vars()\n    File \"/tmp/ipykernel_7363/3443802096.py\", line 31, in dummy_vars  **\n        return [self.variables[0]]\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 2066, in variables\n        return self.weights\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 2829, in weights\n        return self._dedup_weights(self._undeduplicated_weights)\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 2834, in _undeduplicated_weights\n        self._assert_weights_created()\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/sequential.py\", line 472, in _assert_weights_created\n        super(functional.Functional, self)._assert_weights_created()  # pylint: disable=bad-super-call\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 3027, in _assert_weights_created\n        raise ValueError(f'Weights for model {self.name} have not yet been '\n\n    ValueError: Weights for model sequential_79 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.\n"
     ]
    }
   ],
   "source": [
    "DATASET_SIZE, INPUT_SIZE, OUTPUT_SIZE = 1000, 2, 1\n",
    "BATCH_SIZE = 64\n",
    "data = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform((DATASET_SIZE, INPUT_SIZE)), tf.random.uniform((DATASET_SIZE, OUTPUT_SIZE)))\n",
    "    ).batch(BATCH_SIZE)\n",
    "\n",
    "class TM(tf.keras.Model):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(OUTPUT_SIZE)])\n",
    "# compile model\n",
    "model.compile(optimizer=\n",
    "    tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mae\"])\n",
    "\n",
    "def get_gate_on_vars(self):\n",
    "    return self.variables[:2]\n",
    "\n",
    "bind(model, get_gate_on_vars)\n",
    "\n",
    "def gate_config(self):\n",
    "    return self.epochs % 2 == 1\n",
    "\n",
    "def delay_config(self):\n",
    "    return self.epochs > 5\n",
    "\n",
    "def dummy_vars(self):\n",
    "    return [self.variables[0]]\n",
    "\n",
    "config = {\n",
    "    \"gate\": {\n",
    "        \"cond\": gate_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanAbsoluteError(),\n",
    "            \"clipping\": (-0.2, 0.3),\n",
    "            # \"variables\": dummy_vars\n",
    "        },\n",
    "        False: {}\n",
    "        # False: {\n",
    "        #     # \"loss\": tf.keras.losses.MeanAbsoluteError(), \n",
    "        # }\n",
    "    },\n",
    "    \"delay\": {\n",
    "        \"cond\": delay_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanSquaredError(), \n",
    "        },\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "lcc = LoopControlerCallback(config)\n",
    "# start training\n",
    "history = model.fit(data, epochs = 10, verbose = 1,\n",
    "    callbacks=[lcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential_79 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mvariables\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/base_layer.py:2066\u001b[0m, in \u001b[0;36mLayer.variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2054\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_generate_docs\n\u001b[1;32m   2055\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvariables\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2056\u001b[0m   \u001b[39m\"\"\"Returns the list of all layer variables/weights.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \n\u001b[1;32m   2058\u001b[0m \u001b[39m  Alias of `self.weights`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[39m    A list of variables.\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2066\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py:2829\u001b[0m, in \u001b[0;36mModel.weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2819\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2820\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2821\u001b[0m   \u001b[39m\"\"\"Returns the list of all layer variables/weights.\u001b[39;00m\n\u001b[1;32m   2822\u001b[0m \n\u001b[1;32m   2823\u001b[0m \u001b[39m  Note: This will not track the weights of nested `tf.Modules` that are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2827\u001b[0m \u001b[39m    A list of variables.\u001b[39;00m\n\u001b[1;32m   2828\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2829\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dedup_weights(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_undeduplicated_weights)\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py:2834\u001b[0m, in \u001b[0;36mModel._undeduplicated_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2831\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2832\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_undeduplicated_weights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2833\u001b[0m   \u001b[39m\"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2834\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_assert_weights_created()\n\u001b[1;32m   2835\u001b[0m   weights \u001b[39m=\u001b[39m []\n\u001b[1;32m   2836\u001b[0m   \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_tracked_trackables:\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/sequential.py:472\u001b[0m, in \u001b[0;36mSequential._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39m# When the graph has not been initialized, use the Model's implementation to\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# to check if the weights has been created.\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[39msuper\u001b[39;49m(functional\u001b[39m.\u001b[39;49mFunctional, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_assert_weights_created()\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py:3027\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3019\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   3021\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mbuild\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3022\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39m!=\u001b[39m Model \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3023\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt):\n\u001b[1;32m   3024\u001b[0m   \u001b[39m# For any model that has customized build() method but hasn't\u001b[39;00m\n\u001b[1;32m   3025\u001b[0m   \u001b[39m# been invoked yet, this will cover both sequential and subclass model.\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m   \u001b[39m# Also make sure to exclude Model class itself which has build() defined.\u001b[39;00m\n\u001b[0;32m-> 3027\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWeights for model \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m have not yet been \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3028\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mcreated. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3029\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mWeights are created when the Model is first called on \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3030\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Weights for model sequential_79 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential_59 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mget_gate_on_vars()\n",
      "\u001b[1;32m/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb Cell 8\u001b[0m in \u001b[0;36mdummy_vars\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X26sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdummy_vars\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariables[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/base_layer.py:2066\u001b[0m, in \u001b[0;36mLayer.variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2054\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_generate_docs\n\u001b[1;32m   2055\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvariables\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2056\u001b[0m   \u001b[39m\"\"\"Returns the list of all layer variables/weights.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \n\u001b[1;32m   2058\u001b[0m \u001b[39m  Alias of `self.weights`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[39m    A list of variables.\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2066\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py:2829\u001b[0m, in \u001b[0;36mModel.weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2819\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2820\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2821\u001b[0m   \u001b[39m\"\"\"Returns the list of all layer variables/weights.\u001b[39;00m\n\u001b[1;32m   2822\u001b[0m \n\u001b[1;32m   2823\u001b[0m \u001b[39m  Note: This will not track the weights of nested `tf.Modules` that are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2827\u001b[0m \u001b[39m    A list of variables.\u001b[39;00m\n\u001b[1;32m   2828\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2829\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dedup_weights(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_undeduplicated_weights)\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py:2834\u001b[0m, in \u001b[0;36mModel._undeduplicated_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2831\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2832\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_undeduplicated_weights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2833\u001b[0m   \u001b[39m\"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2834\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_assert_weights_created()\n\u001b[1;32m   2835\u001b[0m   weights \u001b[39m=\u001b[39m []\n\u001b[1;32m   2836\u001b[0m   \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_tracked_trackables:\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/sequential.py:472\u001b[0m, in \u001b[0;36mSequential._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39m# When the graph has not been initialized, use the Model's implementation to\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# to check if the weights has been created.\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[39msuper\u001b[39;49m(functional\u001b[39m.\u001b[39;49mFunctional, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_assert_weights_created()\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py:3027\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3019\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   3021\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mbuild\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3022\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39m!=\u001b[39m Model \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3023\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt):\n\u001b[1;32m   3024\u001b[0m   \u001b[39m# For any model that has customized build() method but hasn't\u001b[39;00m\n\u001b[1;32m   3025\u001b[0m   \u001b[39m# been invoked yet, this will cover both sequential and subclass model.\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m   \u001b[39m# Also make sure to exclude Model class itself which has build() defined.\u001b[39;00m\n\u001b[0;32m-> 3027\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWeights for model \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m have not yet been \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3028\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mcreated. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3029\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mWeights are created when the Model is first called on \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3030\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Weights for model sequential_59 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "model.get_gate_on_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential_45 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mget_gate_on_vars()\n",
      "\u001b[1;32m/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb Cell 7\u001b[0m in \u001b[0;36mdummy_vars\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdummy_vars\u001b[39m(\u001b[39mself\u001b[39m,):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariables[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/base_layer.py:2066\u001b[0m, in \u001b[0;36mLayer.variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2053\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2054\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_generate_docs\n\u001b[1;32m   2055\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvariables\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2056\u001b[0m   \u001b[39m\"\"\"Returns the list of all layer variables/weights.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \n\u001b[1;32m   2058\u001b[0m \u001b[39m  Alias of `self.weights`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[39m    A list of variables.\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2066\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py:2829\u001b[0m, in \u001b[0;36mModel.weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2819\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2820\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mweights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2821\u001b[0m   \u001b[39m\"\"\"Returns the list of all layer variables/weights.\u001b[39;00m\n\u001b[1;32m   2822\u001b[0m \n\u001b[1;32m   2823\u001b[0m \u001b[39m  Note: This will not track the weights of nested `tf.Modules` that are not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2827\u001b[0m \u001b[39m    A list of variables.\u001b[39;00m\n\u001b[1;32m   2828\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2829\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dedup_weights(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_undeduplicated_weights)\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py:2834\u001b[0m, in \u001b[0;36mModel._undeduplicated_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2831\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m   2832\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_undeduplicated_weights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2833\u001b[0m   \u001b[39m\"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2834\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_assert_weights_created()\n\u001b[1;32m   2835\u001b[0m   weights \u001b[39m=\u001b[39m []\n\u001b[1;32m   2836\u001b[0m   \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_tracked_trackables:\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/sequential.py:472\u001b[0m, in \u001b[0;36mSequential._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39m# When the graph has not been initialized, use the Model's implementation to\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m# to check if the weights has been created.\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[39msuper\u001b[39;49m(functional\u001b[39m.\u001b[39;49mFunctional, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_assert_weights_created()\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py:3027\u001b[0m, in \u001b[0;36mModel._assert_weights_created\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3019\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   3021\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mbuild\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3022\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39m!=\u001b[39m Model \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3023\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt):\n\u001b[1;32m   3024\u001b[0m   \u001b[39m# For any model that has customized build() method but hasn't\u001b[39;00m\n\u001b[1;32m   3025\u001b[0m   \u001b[39m# been invoked yet, this will cover both sequential and subclass model.\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m   \u001b[39m# Also make sure to exclude Model class itself which has build() defined.\u001b[39;00m\n\u001b[0;32m-> 3027\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWeights for model \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m have not yet been \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3028\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mcreated. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3029\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mWeights are created when the Model is first called on \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   3030\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39minputs or `build()` is called with an `input_shape`.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Weights for model sequential_45 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "model.get_gate_on_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_self_setattr_tracking',\n",
       " '_is_model_for_instrumentation',\n",
       " '_instrumented_keras_api',\n",
       " '_instrumented_keras_layer_class',\n",
       " '_instrumented_keras_model_class',\n",
       " '_trainable',\n",
       " '_stateful',\n",
       " 'built',\n",
       " '_input_spec',\n",
       " '_build_input_shape',\n",
       " '_saved_model_inputs_spec',\n",
       " '_saved_model_arg_spec',\n",
       " '_supports_masking',\n",
       " '_name',\n",
       " '_activity_regularizer',\n",
       " '_trainable_weights',\n",
       " '_non_trainable_weights',\n",
       " '_updates',\n",
       " '_thread_local',\n",
       " '_callable_losses',\n",
       " '_losses',\n",
       " '_metrics',\n",
       " '_metrics_lock',\n",
       " '_dtype_policy',\n",
       " '_compute_dtype_object',\n",
       " '_autocast',\n",
       " '_self_tracked_trackables',\n",
       " '_inbound_nodes_value',\n",
       " '_outbound_nodes_value',\n",
       " '_expects_training_arg',\n",
       " '_default_training_arg',\n",
       " '_expects_mask_arg',\n",
       " '_dynamic',\n",
       " '_initial_weights',\n",
       " '_auto_track_sub_layers',\n",
       " '_preserve_input_structure_in_config',\n",
       " '_name_scope_on_declaration',\n",
       " '_captured_weight_regularizer',\n",
       " '_is_graph_network',\n",
       " 'inputs',\n",
       " 'outputs',\n",
       " 'input_names',\n",
       " 'output_names',\n",
       " '_compute_output_and_mask_jointly',\n",
       " '_distribution_strategy',\n",
       " '_cluster_coordinator',\n",
       " 'test_function',\n",
       " 'predict_function',\n",
       " '_compiled_trainable_state',\n",
       " '_training_state',\n",
       " '_self_unconditional_checkpoint_dependencies',\n",
       " '_self_unconditional_dependency_names',\n",
       " '_self_unconditional_deferred_dependencies',\n",
       " '_self_update_uid',\n",
       " '_self_name_based_restores',\n",
       " '_self_saveable_object_factories',\n",
       " '_checkpoint',\n",
       " '_steps_per_execution',\n",
       " '_train_counter',\n",
       " '_test_counter',\n",
       " '_predict_counter',\n",
       " '_base_model_initialized',\n",
       " '_layout_map',\n",
       " '_inferred_input_shape',\n",
       " '_has_explicit_input_shape',\n",
       " '_input_dtype',\n",
       " '_layer_call_argspecs',\n",
       " '_created_nodes',\n",
       " '_graph_initialized',\n",
       " '_use_legacy_deferred_behavior',\n",
       " '_obj_reference_counts_dict',\n",
       " '_run_eagerly',\n",
       " 'optimizer',\n",
       " 'compiled_loss',\n",
       " 'compiled_metrics',\n",
       " '_is_compiled',\n",
       " 'loss',\n",
       " '_jit_compile',\n",
       " 'history',\n",
       " 'stop_training',\n",
       " 'train_tf_function',\n",
       " 'train_function',\n",
       " 'cv_names',\n",
       " 'gate',\n",
       " 'delay',\n",
       " 'train_step',\n",
       " 'gate_on_vars',\n",
       " 'get_gate_on_vars',\n",
       " 'gate_on',\n",
       " 'gate_off',\n",
       " 'delay_on',\n",
       " '__module__',\n",
       " '__doc__',\n",
       " '__init__',\n",
       " 'layers',\n",
       " 'add',\n",
       " 'pop',\n",
       " '_build_graph_network_for_inferred_shape',\n",
       " 'build',\n",
       " 'call',\n",
       " 'compute_output_shape',\n",
       " 'compute_mask',\n",
       " 'get_config',\n",
       " 'from_config',\n",
       " 'input_spec',\n",
       " '_trackable_saved_model_saver',\n",
       " '_is_layer_name_unique',\n",
       " '_assert_weights_created',\n",
       " '_keras_api_names',\n",
       " '_keras_api_names_v1',\n",
       " '_TF_MODULE_IGNORED_PROPERTIES',\n",
       " '_init_graph_network',\n",
       " 'input',\n",
       " 'input_shape',\n",
       " 'output',\n",
       " 'output_shape',\n",
       " '_set_output_names',\n",
       " '_layer_checkpoint_dependencies',\n",
       " '_trackable_children',\n",
       " '_lookup_dependency',\n",
       " '_handle_deferred_layer_dependencies',\n",
       " '_should_compute_mask',\n",
       " '_init_set_name',\n",
       " '_run_internal_graph',\n",
       " '_flatten_to_reference_inputs',\n",
       " '_conform_to_reference_input',\n",
       " '_validate_graph_inputs_and_outputs',\n",
       " '_insert_layers',\n",
       " '_compute_tensor_usage_count',\n",
       " '_graph_network_add_loss',\n",
       " '_graph_network_add_metric',\n",
       " '_get_save_spec',\n",
       " '_SCALAR_UPRANKING_ON',\n",
       " '__new__',\n",
       " '_init_batch_counters',\n",
       " '__setattr__',\n",
       " '__reduce__',\n",
       " '__deepcopy__',\n",
       " '__copy__',\n",
       " '__call__',\n",
       " 'compile',\n",
       " '_get_optimizer',\n",
       " '_reset_compile_cache',\n",
       " '_configure_steps_per_execution',\n",
       " 'metrics',\n",
       " 'metrics_names',\n",
       " 'distribute_strategy',\n",
       " 'run_eagerly',\n",
       " '_validate_target_and_loss',\n",
       " 'compute_loss',\n",
       " 'compute_metrics',\n",
       " 'make_train_function',\n",
       " 'fit',\n",
       " 'test_step',\n",
       " 'make_test_function',\n",
       " 'evaluate',\n",
       " 'predict_step',\n",
       " 'make_predict_function',\n",
       " 'predict',\n",
       " 'reset_metrics',\n",
       " 'train_on_batch',\n",
       " 'test_on_batch',\n",
       " 'predict_on_batch',\n",
       " 'fit_generator',\n",
       " 'evaluate_generator',\n",
       " 'predict_generator',\n",
       " 'trainable_weights',\n",
       " 'non_trainable_weights',\n",
       " 'get_weights',\n",
       " 'save',\n",
       " 'save_weights',\n",
       " 'load_weights',\n",
       " '_updated_config',\n",
       " 'to_json',\n",
       " 'to_yaml',\n",
       " 'reset_states',\n",
       " 'state_updates',\n",
       " 'weights',\n",
       " '_undeduplicated_weights',\n",
       " 'summary',\n",
       " 'get_layer',\n",
       " '_set_save_spec',\n",
       " 'save_spec',\n",
       " '_check_call_args',\n",
       " '_validate_compile',\n",
       " '_maybe_load_initial_epoch_from_ckpt',\n",
       " '_maybe_load_initial_step_from_ckpt',\n",
       " '_assert_compile_was_called',\n",
       " '_check_sample_weight_warning',\n",
       " '_set_inputs',\n",
       " '_should_eval',\n",
       " '_get_compile_args',\n",
       " '_get_callback_model',\n",
       " '_in_multi_worker_mode',\n",
       " '_compile_was_called',\n",
       " 'add_weight',\n",
       " 'compute_output_signature',\n",
       " '_get_unnested_name_scope',\n",
       " 'dtype',\n",
       " 'name',\n",
       " 'supports_masking',\n",
       " 'dynamic',\n",
       " 'stateful',\n",
       " 'trainable',\n",
       " 'activity_regularizer',\n",
       " 'updates',\n",
       " 'losses',\n",
       " 'add_loss',\n",
       " 'add_metric',\n",
       " 'add_update',\n",
       " 'set_weights',\n",
       " 'finalize_state',\n",
       " 'get_input_mask_at',\n",
       " 'get_output_mask_at',\n",
       " 'input_mask',\n",
       " 'output_mask',\n",
       " 'get_input_shape_at',\n",
       " 'get_output_shape_at',\n",
       " 'get_input_at',\n",
       " 'get_output_at',\n",
       " 'count_params',\n",
       " 'dtype_policy',\n",
       " 'compute_dtype',\n",
       " 'variable_dtype',\n",
       " 'inbound_nodes',\n",
       " 'outbound_nodes',\n",
       " 'variables',\n",
       " 'trainable_variables',\n",
       " 'non_trainable_variables',\n",
       " 'add_variable',\n",
       " '_must_restore_from_config',\n",
       " '_get_cell_name',\n",
       " '_instrument_layer_creation',\n",
       " '_add_trackable',\n",
       " '_clear_losses',\n",
       " '_keras_tensor_symbolic_call',\n",
       " '_infer_output_signature',\n",
       " '_functional_construction_call',\n",
       " '_set_training_mode',\n",
       " '_autographed_call',\n",
       " '_inbound_nodes',\n",
       " '_outbound_nodes',\n",
       " '_set_dtype_policy',\n",
       " '_compute_dtype',\n",
       " '_maybe_cast_inputs',\n",
       " '_should_cast_single_input',\n",
       " '_cast_single_input',\n",
       " '_dtype',\n",
       " '_name_scope',\n",
       " '_get_existing_metric',\n",
       " '_handle_weight_regularization',\n",
       " '_handle_activity_regularization',\n",
       " '_set_mask_metadata',\n",
       " '_set_mask_keras_history_checked',\n",
       " '_get_input_masks',\n",
       " '_call_arg_was_passed',\n",
       " '_get_call_arg_value',\n",
       " '_set_call_arg_value',\n",
       " '_set_connectivity_metadata',\n",
       " '_get_node_attribute_at_index',\n",
       " '_maybe_build',\n",
       " '_get_trainable_state',\n",
       " '_set_trainable_state',\n",
       " '_obj_reference_counts',\n",
       " '_maybe_create_attribute',\n",
       " '__delattr__',\n",
       " '_gather_children_attribute',\n",
       " '_flatten_layers',\n",
       " '_flatten_modules',\n",
       " '_is_layer',\n",
       " '_init_call_fn_args',\n",
       " '_call_full_argspec',\n",
       " '_call_fn_args',\n",
       " '_call_fn_arg_defaults',\n",
       " '_call_fn_arg_positions',\n",
       " '_call_accepts_kwargs',\n",
       " '_eager_losses',\n",
       " '_dedup_weights',\n",
       " '_split_out_first_arg',\n",
       " '_object_identifier',\n",
       " '_tracking_metadata',\n",
       " '_use_input_spec_as_call_signature',\n",
       " '__getstate__',\n",
       " '__setstate__',\n",
       " 'name_scope',\n",
       " 'submodules',\n",
       " '_flatten',\n",
       " 'with_name_scope',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " '_no_dependency',\n",
       " '_delete_tracking',\n",
       " '_add_trackable_child',\n",
       " '_setattr_tracking',\n",
       " '_update_uid',\n",
       " '_unconditional_checkpoint_dependencies',\n",
       " '_unconditional_dependency_names',\n",
       " '_name_based_restores',\n",
       " '_maybe_initialize_trackable',\n",
       " '_name_based_attribute_restore',\n",
       " '_checkpoint_dependencies',\n",
       " '_deferred_dependencies',\n",
       " '_add_variable_with_custom_getter',\n",
       " '_preload_simple_restoration',\n",
       " '_track_trackable',\n",
       " '_handle_deferred_dependencies',\n",
       " '_restore_from_checkpoint_position',\n",
       " '_single_restoration_from_checkpoint_position',\n",
       " '_gather_saveables_for_checkpoint',\n",
       " '_serialize_to_tensors',\n",
       " '_restore_from_tensors',\n",
       " '_list_extra_dependencies_for_serialization',\n",
       " '_list_functions_for_serialization',\n",
       " '_map_resources',\n",
       " '_serialize_to_proto',\n",
       " '_deserialize_from_proto',\n",
       " '_deserialization_dependencies',\n",
       " '_get_legacy_saved_model_children',\n",
       " '_export_to_saved_model_graph',\n",
       " '__dict__',\n",
       " '__weakref__',\n",
       " '__repr__',\n",
       " '__hash__',\n",
       " '__str__',\n",
       " '__getattribute__',\n",
       " '__lt__',\n",
       " '__le__',\n",
       " '__eq__',\n",
       " '__ne__',\n",
       " '__gt__',\n",
       " '__ge__',\n",
       " '__reduce_ex__',\n",
       " '__subclasshook__',\n",
       " '__init_subclass__',\n",
       " '__format__',\n",
       " '__sizeof__',\n",
       " '__dir__',\n",
       " '__class__']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        lscope = {\n",
    "            **locals(), \n",
    "            **fn_config\n",
    "            }\n",
    "        \n",
    "        vars_name = self._get_name_for_sleve_step_variables(fn_name)\n",
    "        function_body = f\"\"\"\n",
    "@tf.function\n",
    "def {fn_name}(self, data):\n",
    "    x, y = data        \n",
    "        \"\"\"\n",
    "\n",
    "        if fn_config[\"variables\"]:\n",
    "            lscope[\"action_vars\"] = getattr(self.model, vars_name)\n",
    "            lscope[\"get_slave_vars\"] = lscope[\"variables\"]\n",
    "            function_body += f\"\"\"\n",
    "    if not self.{vars_name}:\n",
    "        self.{vars_name}=get_slave_vars()\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        for g in self.{vars_name}:\n",
    "            tape.watch(g)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            function_body += \"\"\"\n",
    "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
    "            \"\"\"\n",
    "\n",
    "        function_body += \"\"\"\n",
    "        logits = self(x, training=True)\n",
    "        loss_value = loss(y, logits)\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip({clipping_grads}, tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "    return loss_value\n",
    "\"\"\".format(**{\n",
    "    \"clipping_grads\": \"[tf.clip_by_value(g, {clip_low}, {clip_high}) for g in grads]\".format(**{\n",
    "        \"clip_low\": fn_config[\"clipping\"][0], \"clip_high\": fn_config[\"clipping\"][1]\n",
    "        }) if fn_config[\"clipping\"] else \"grads\",\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        function_body = \"\"\"\n",
    "@tf.function\n",
    "def {fn_name}(self, data):\n",
    "    {variables}\n",
    "    x, y = data\n",
    "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
    "        logits = self(x, training=True)\n",
    "        loss_value = loss(y, logits)\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip({clipping_grads}, tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "    return loss_value\n",
    "\"\"\".format(**{\n",
    "    \"fn_name\": fn_name,\n",
    "    \"clipping_grads\": \"[tf.clip_by_value(g, {clip_low}, {clip_high}) for g in grads]\".format(**{\"clip_low\": fn_config[\"clipping\"][0], \"clip_high\": fn_config[\"clipping\"][1]}) if fn_config[\"clipping\"] else \"grads\",\n",
    "    \"variables\": \"if not self.{var_names}:\\n        self.{var_names}=variables()\\n\".format(**{\n",
    "        \"variables\" : lscope[\"variables\"], \"var_names\" : vars_name\n",
    "        }) if lscope[\"variables\"] else \"\",\n",
    "    \"test\" : self._get_name_for_sleve_step_variables(fn_name)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopControlerCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, config: int, default_in_branch: Dict[str, Any] = None, verbose: bool = True, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super(LoopControlerCallback, self).__init__(*args, **kwargs)\n",
    "        self.default_in_branch: Dict[str, Any] = default_in_branch if default_in_branch else {\n",
    "                \"loss\": None, \n",
    "                \"clipping\": None,\n",
    "                \"variables\": None,\n",
    "                \"exclud_var\": None,\n",
    "            }\n",
    "        self.config: Dict[str, Any] = config\n",
    "        self.verbose: bool = verbose\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"Function called directely before training. It is executed on each call of model.fit with this callback.\n",
    "            Inside the scope of this funciton we can access model on which this callback works: self.model.\n",
    "\n",
    "        Args:\n",
    "            logs (_type_, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        # meta attributes for building conditions\n",
    "        self.epochs: int = 0\n",
    "        self.batches: int = 0\n",
    "        self.batches_in_epoch: int = 0\n",
    "        self.last_loss: int = 0.0\n",
    "        self.history: List[Any] = []\n",
    "\n",
    "        # control variables list of names \n",
    "        self.model.cv_names = []\n",
    "        # \n",
    "        self.c_conds = {}\n",
    "        \n",
    "        # if loss in default branch is None, then use compiled loss\n",
    "        if not self.default_in_branch[\"loss\"]:\n",
    "            self.default_in_branch[\"loss\"] = self.model.compiled_loss\n",
    "        \n",
    "        # extend config with validation step\n",
    "        self.config = self._extend_actions_config(self.config)\n",
    "        # bind control variables to the model\n",
    "        self._bind_slave_variables(self.config)\n",
    "        # bind slave control confition to the callback\n",
    "        self._bind_slave_conditions(self.config)\n",
    "        # bind master train_step to model\n",
    "        self._bind_master_step(self.config)\n",
    "        # bind placeholders for list of variables for slave steps\n",
    "        self._bind_slaves_steps_dif_variables(self.config)\n",
    "        # bind slave train_steps to model\n",
    "        self._bin_slaves_steps()\n",
    "\n",
    "    def _get_slave_variable_name(self, action_name: str) -> str:\n",
    "        \"\"\"Get control variable from action name\n",
    "\n",
    "        Args:\n",
    "            action_name (str): name of the action slave train step\n",
    "\n",
    "        Returns:\n",
    "            str: converted name of the action slave train step\n",
    "        \"\"\"\n",
    "        return f\"{action_name}_slave_var\"\n",
    "\n",
    "    def _get_slave_condition_names(self, condition_name: str) -> Tuple[str, str]:\n",
    "        return (f\"{condition_name}_off\", f\"{condition_name}_on\")\n",
    "\n",
    "    def _bind_slave_variables(self, config) -> None:\n",
    "        for cv_name in config.keys():\n",
    "            name = self._get_slave_variable_name(cv_name)\n",
    "            setattr(self.model, name, tf.Variable(False, trainable=False))\n",
    "            self.model.cv_names.append(name)\n",
    "\n",
    "    def _get_slave_condition_name(self, action_name: str) -> str:\n",
    "        \"\"\"Get control variable from action name\n",
    "\n",
    "        Args:\n",
    "            action_name (str): name of the action slave train step\n",
    "\n",
    "        Returns:\n",
    "            str: converted name of the action slave train step\n",
    "        \"\"\"\n",
    "        return f\"{action_name}_slave_cond\" \n",
    "\n",
    "    def _bind_slave_conditions(self, config) -> None:\n",
    "        for action_name, action_config in config.items():\n",
    "            name = self._get_slave_condition_name(action_name)\n",
    "            bind(self, action_config[\"cond\"], name)\n",
    "            self.c_conds[self._get_slave_variable_name(action_name)] = name\n",
    "\n",
    "    def _bind_master_step(self, config) -> None:\n",
    "        lscope = locals()\n",
    "        def _get_losses(config: Dict[str, Any]) -> str:\n",
    "            def _substep_exec(name: str, conf: Dict[str, Any], on: bool) -> str:\n",
    "                if on:\n",
    "                    return f\"self.{name}_on(data)\" if True in conf else \"0.0\"\n",
    "                else:\n",
    "                    return f\"self.{name}_off(data)\" if False in conf else \"0.0\"\n",
    "\n",
    "            return \"{\" + \",\".join([\n",
    "                        f\"'loss_{an}' : tf.cond(self.{self._get_slave_variable_name(an)}, lambda: {_substep_exec(an, ac, True)}, lambda: {_substep_exec(an, ac, False)})\"\n",
    "                        for an, ac in config.items()\n",
    "                    ]) + \"}\"\n",
    "\n",
    "        lscope = locals()\n",
    "        function_body = \"\"\"\n",
    "@tf.function\n",
    "def train_step(self, data):\n",
    "    loss = {losses_config}\n",
    "    metrics = {{m.name : m.result() for m in self.metrics}}\n",
    "    \n",
    "    control_states = {{\n",
    "            c_name: tf.cond(\n",
    "                getattr(self, c_name),\n",
    "                lambda: tf.constant(True),\n",
    "                lambda: tf.constant(False),\n",
    "            )\n",
    "            for c_name in self.cv_names\n",
    "    }}\n",
    "    \n",
    "    return {{**loss, **metrics, **control_states}}\n",
    "\"\"\".format(\n",
    "            **{\"losses_config\": _get_losses(config)}\n",
    "        )\n",
    "        if self.verbose:\n",
    "            print(\"-------------------MASTER STEP-------------------\")\n",
    "            print(function_body)\n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[\"train_step\"])\n",
    "\n",
    "    def _get_name_for_sleve_step_variables(self, name, on = None):\n",
    "        if on == None:\n",
    "            return f\"{name}_vars\"\n",
    "        return f\"{name}_{'on' if on else 'off'}_vars\"\n",
    "\n",
    "    def _bind_slaves_steps_dif_variables(self, config):\n",
    "        for action_name, action_config in config.items():\n",
    "            if action_config[True] and \"variables\" in action_config[True] and action_config[True][\"variables\"]:\n",
    "                # we have to bind None because variables before trainign are not initialised\n",
    "                setattr(self.model, self._get_name_for_sleve_step_variables(action_name, True), action_config[True][\"variables\"])\n",
    "\n",
    "\n",
    "            if False in action_config and \"variables\" in action_config[False] and action_config[False][\"variables\"]:\n",
    "                # we have to bind None because variables before trainign are not initialised\n",
    "                setattr(self.model, self._get_name_for_sleve_step_variables(action_name, False), action_config[False][\"variables\"])\n",
    "\n",
    "    def _bin_slaves_steps(self) -> None:\n",
    "        if self.verbose:\n",
    "            print(\"-------------------SLAVE STEPS-------------------\\n\")\n",
    "        for action_name, action_config in self.config.items():\n",
    "            off_step_name, on_step_name = self._get_slave_condition_names(action_name)\n",
    "            if True in action_config:\n",
    "                self._bind_slave_step(on_step_name, action_config[True])\n",
    "            if False in action_config:\n",
    "                self._bind_slave_step(off_step_name, action_config[False])\n",
    "\n",
    "    def _bind_slave_step(self, fn_name: str, fn_config: Dict[str, Any]) -> None:\n",
    "        lscope = {\n",
    "            **locals(), \n",
    "            **fn_config,\n",
    "            **{\"vars_control_name\": self._get_name_for_sleve_step_variables(fn_name)}\n",
    "            }\n",
    "        # print(getattr(self.model, vars_control_name)())\n",
    "        function_body = \"\"\"\n",
    "@tf.function\n",
    "def {fn_name}(self, data):\n",
    "    x, y = data\n",
    "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
    "        logits = self(x, training=True)\n",
    "        loss_value = loss(y, logits)\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip({clipping_grads}, tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "\"\"\".format(**{\n",
    "    \"fn_name\": fn_name,\n",
    "    \"clipping_grads\": \"[tf.clip_by_value(g, {clip_low}, {clip_high}) for g in grads]\".format(**{\"clip_low\": fn_config[\"clipping\"][0], \"clip_high\": fn_config[\"clipping\"][1]}) if fn_config[\"clipping\"] else \"grads\",\n",
    "    \"variables\": \"if not getattr(self.model, vars_control_name):\\n      getattr(self.model, vars_control_name)=variables()\\n\" if fn_config[\"variables\"] else \"\",\n",
    "    \"test\" : self._get_name_for_sleve_step_variables(fn_name)\n",
    "\n",
    "})\n",
    "        if self.verbose:\n",
    "            print(f\"-------------------{fn_name}-------------------\")\n",
    "            print(fn_config[\"variables\"])\n",
    "            print(function_body)\n",
    "            \n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[fn_name])\n",
    "\n",
    "    def _extend_actions_config(self, config: Dict[str, Any]) -> None:       \n",
    "        \"\"\"Extend and validate config file. Fill missing fields based on the default_in_branch.\n",
    "\n",
    "        Args:\n",
    "            config (Dict[str, Any]): Configuration to control model training\n",
    "        \"\"\"\n",
    "\n",
    "        def _validate_action_config(action_name: str, action_config: Dict[str, Any]) -> None:\n",
    "            \"\"\"Validate model training configuration.\n",
    "\n",
    "            Args:\n",
    "                action_name (str): name of the action slave train step\n",
    "                action_config (Dict[str, Any]): configuration of the action slave train step\n",
    "\n",
    "            Raises:\n",
    "                ValueError: Missing controlable cond\n",
    "                ValueError: Missing branch configuration for true/false after cond\n",
    "            \"\"\"\n",
    "\n",
    "            Warning(f\"------Validating Configuration for {action_name}------\")\n",
    "            if action_config == {}:\n",
    "                Warning(\n",
    "                    f\"{action_name} has empty body. Condition and False or True branch must be implemented.\\n It's ignored in furhter computations\"\n",
    "                )\n",
    "            if (True not in action_config) and (False not in action_config):\n",
    "                raise ValueError(\n",
    "                    f\"{action_name} has no False or True branch implemented\"\n",
    "                )\n",
    "            if \"cond\" not in action_config:\n",
    "                raise ValueError(f\"{action_name} has no condition implemented.\")\n",
    "\n",
    "        extended_config = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            _validate_action_config(action_name, action_config)\n",
    "            extended_config[action_name] = {\"cond\": action_config[\"cond\"]}\n",
    "            if True in action_config:\n",
    "                extended_config[action_name][True] = {**self.default_in_branch, **action_config[True]}\n",
    "            if False in action_config:\n",
    "                extended_config[action_name][False] = {**self.default_in_branch, **action_config[False]}\n",
    "\n",
    "        return extended_config\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs) -> None:\n",
    "        self.epochs += 1\n",
    "        \"\"\"Control gating variable from the level of callback which can work on epoch/batch level.\"\"\"\n",
    "        # tf.variable.assign is different than tf.variable = <sth>. The second option is compiled to static\n",
    "        # value in TF graph of computation as the result of @tf.function decorators in LoopControlableModel\n",
    "        # print(self.c_conds)\n",
    "        print(self.c_conds)\n",
    "        for control_variable_name, control_function_name in self.c_conds.items():\n",
    "            print(getattr(self.model, control_variable_name))\n",
    "            print(getattr(self, control_function_name)())\n",
    "\n",
    "            getattr(self.model, control_variable_name).assign(\n",
    "                getattr(self, control_function_name)()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------MASTER STEP-------------------\n",
      "\n",
      "@tf.function\n",
      "def train_step(self, data):\n",
      "    loss = {'loss_gate' : tf.cond(self.gate_slave_var, lambda: self.gate_on(data), lambda: self.gate_off(data)),'loss_delay' : tf.cond(self.delay_slave_var, lambda: self.delay_on(data), lambda: 0.0)}\n",
      "    metrics = {m.name : m.result() for m in self.metrics}\n",
      "    \n",
      "    control_states = {\n",
      "            c_name: tf.cond(\n",
      "                getattr(self, c_name),\n",
      "                lambda: tf.constant(True),\n",
      "                lambda: tf.constant(False),\n",
      "            )\n",
      "            for c_name in self.cv_names\n",
      "    }\n",
      "    \n",
      "    return {**loss, **metrics, **control_states}\n",
      "\n",
      "-------------------SLAVE STEPS-------------------\n",
      "\n",
      "-------------------gate_on-------------------\n",
      "<function dummy_vars at 0x7ff4986234c0>\n",
      "\n",
      "@tf.function\n",
      "def gate_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.2, 0.3) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "\n",
      "-------------------gate_off-------------------\n",
      "None\n",
      "\n",
      "@tf.function\n",
      "def gate_off(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "\n",
      "-------------------delay_on-------------------\n",
      "None\n",
      "\n",
      "@tf.function\n",
      "def delay_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "\n",
      "{'gate_slave_var': 'gate_slave_cond', 'delay_slave_var': 'delay_slave_cond'}\n",
      "<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False>\n",
      "True\n",
      "<tf.Variable 'Variable:0' shape=() dtype=bool, numpy=False>\n",
      "False\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"<string>\", line 4, in train_step\n        \n\n    TypeError: true_fn and false_fn arguments to tf.cond must have the same number, type, and overall structure of return values.\n    \n    true_fn output: Tensor(\"cond_1/Identity_1:0\", shape=(), dtype=bool)\n    false_fn output: Tensor(\"cond_1/Identity:0\", shape=(), dtype=float32)\n    \n    Error details:\n    Tensor(\"cond_1/Identity_1:0\", shape=(), dtype=bool) and Tensor(\"cond_1/Identity:0\", shape=(), dtype=float32) have different types\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X16sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m lcc \u001b[39m=\u001b[39m LoopControlerCallback(config)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X16sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# start training\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X16sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(data, epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/filip/workspace/tf/tf2-loop-control/tf2-loop-controll.ipynb#X16sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[lcc])\n",
      "File \u001b[0;32m~/workspace/tf/venv/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filerihwkru0.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m<string>:4\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/filip/workspace/tf/venv/lib/python3.8/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"<string>\", line 4, in train_step\n        \n\n    TypeError: true_fn and false_fn arguments to tf.cond must have the same number, type, and overall structure of return values.\n    \n    true_fn output: Tensor(\"cond_1/Identity_1:0\", shape=(), dtype=bool)\n    false_fn output: Tensor(\"cond_1/Identity:0\", shape=(), dtype=float32)\n    \n    Error details:\n    Tensor(\"cond_1/Identity_1:0\", shape=(), dtype=bool) and Tensor(\"cond_1/Identity:0\", shape=(), dtype=float32) have different types\n"
     ]
    }
   ],
   "source": [
    "DATASET_SIZE, INPUT_SIZE, OUTPUT_SIZE = 1000, 2, 1\n",
    "BATCH_SIZE = 64\n",
    "data = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform((DATASET_SIZE, INPUT_SIZE)), tf.random.uniform((DATASET_SIZE, OUTPUT_SIZE)))\n",
    "    ).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Dense(OUTPUT_SIZE)])\n",
    "# compile model\n",
    "model.compile(optimizer=\n",
    "    tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "def gate_config(self):\n",
    "    return self.epochs % 2 == 1\n",
    "\n",
    "def delay_config(self):\n",
    "    return self.epochs > 5\n",
    "\n",
    "def dummy_vars(self,):\n",
    "    return [self.variables[0]]\n",
    "\n",
    "config = {\n",
    "    \"gate\": {\n",
    "        \"cond\": gate_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanAbsoluteError(),\n",
    "            \"clipping\": (-0.2, 0.3),\n",
    "            \"variables\": dummy_vars\n",
    "        },\n",
    "        False: {}\n",
    "        # False: {\n",
    "        #     # \"loss\": tf.keras.losses.MeanAbsoluteError(), \n",
    "        # }\n",
    "    },\n",
    "    \"delay\": {\n",
    "        \"cond\": delay_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanSquaredError(), \n",
    "        },\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "lcc = LoopControlerCallback(config)\n",
    "# start training\n",
    "history = model.fit(data, epochs = 10, verbose = 1,\n",
    "    callbacks=[lcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7ff498651b50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gate': {'cond': <function __main__.gate_config(self)>,\n",
       "  True: {'loss': <keras.losses.MeanSquaredError at 0x7f4c4d08da00>,\n",
       "   'clipping': None,\n",
       "   'var': None,\n",
       "   'exclud_var': None},\n",
       "  False: {'loss': <keras.losses.MeanAbsoluteError at 0x7f4c4d08d490>,\n",
       "   'clipping': None,\n",
       "   'var': None,\n",
       "   'exclud_var': None}},\n",
       " 'delay': {'cond': <function __main__.delay_config(self)>,\n",
       "  True: {'loss': <keras.losses.MeanSquaredError at 0x7f4c4d08ddf0>,\n",
       "   'clipping': None,\n",
       "   'var': None,\n",
       "   'exclud_var': None},\n",
       "  False: None},\n",
       " 'test': {'cond': <function __main__.delay_config(self)>,\n",
       "  True: {'loss': 'self.compiled_loss',\n",
       "   'clipping': None,\n",
       "   'var': None,\n",
       "   'exclud_var': None},\n",
       "  False: None}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"gate\": {\n",
    "        \"cond\": gate_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanSquaredError(), \n",
    "        },\n",
    "        False: {\n",
    "            \"loss\": tf.keras.losses.MeanAbsoluteError(), \n",
    "        }\n",
    "    },\n",
    "    \"delay\": {\n",
    "        \"cond\": delay_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanSquaredError(), \n",
    "        },\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"cond\": delay_config,\n",
    "        True: {}\n",
    "    },\n",
    "    # \"test2\": {\n",
    "    #     \"cond\": delay_config,\n",
    "    # },\n",
    "\n",
    "}\n",
    "\n",
    "def validate_action_config(action_name, action_config) -> bool:\n",
    "        Warning(f\"------Validating Configuration for {action_name}------\")\n",
    "        if action_config == {}:\n",
    "            Warning(f\"{action_name} has empty body. Condition and False or True branch must be implemented.\\n It's ignored in furhter computations\")\n",
    "        if ((True not in action_config) and (True not in action_config)):\n",
    "            raise ValueError(f\"{action_name} has no False or True branch implemented\")\n",
    "        if \"cond\" not in action_config:\n",
    "            raise ValueError(f\"{action_name} has no condition implemented.\")\n",
    "        return True\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "def parse_config(config, default_in_branch = None):\n",
    "    if not default_in_branch:\n",
    "        default_in_branch = {\n",
    "            \"loss\" : \"self.compiled_loss\",\n",
    "            \"clipping\" : None, \n",
    "            \"var\": None,\n",
    "            \"exclud_var\": None,\n",
    "        }\n",
    "\n",
    "    pc = {}\n",
    "    for action_name, action_config in config.items():\n",
    "        _ = validate_action_config(action_name, action_config)\n",
    "        false_branch, true_branch = {}, {}\n",
    "        true_branch = {**default_in_branch,**action_config[True]} if True in action_config else None\n",
    "        false_branch = {**default_in_branch,**action_config[False]} if False in action_config else None\n",
    "        pc[action_name] = {\"cond\": action_config[\"cond\"], True: true_branch, False: false_branch}\n",
    "\n",
    "    return pc \n",
    "            \n",
    "parse_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False not in {False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bool"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from inspect import signature\n",
    "\n",
    "def foo(x: int = 1, y: int = 2) -> bool:\n",
    "    return x > y\n",
    "sig = signature(foo)\n",
    "sig.return_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def gate_on(self, data):\n",
    "    if not self.gate_var:\n",
    "\tgetattr(self, self._get_name_for_sleve_step_variables(gate_on))=variables\n",
    "\n",
    "    x, y = data\n",
    "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
    "        logits = self(x, training=True)\n",
    "        loss_value = loss(y, logits)\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.2, 0.3) for g in grads], tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "class B:\n",
    "    def __init__(self) -> None:\n",
    "        self.x = 2\n",
    "    \n",
    "    def boo(self):\n",
    "        \n",
    "        bb = tf.keras.losses.MeanAbsoluteError()\n",
    "        adict = locals()\n",
    "        # print(adict)\n",
    "        function_body = f\"\"\"\n",
    "def train(self):\n",
    "    print(bb([1,2,3], [3,4,5]))\n",
    "    return self.x\n",
    "\"\"\"\n",
    "        exec(function_body,{**globals(), **adict}, adict)\n",
    "        # print(adict)\n",
    "        # time.sleep(0.1)\n",
    "        bind(self, adict[\"train\"])\n",
    "        print(self.train())\n",
    "        # bind(self, test3)\n",
    "        # bind(self, test)\n",
    "        return True\n",
    "\n",
    "b = B()\n",
    "b.boo()\n",
    "# self.train_substep()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b4ff1917ad7fe489d902c3041fcac2959f3e2431a3acc02677f15f306757aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
