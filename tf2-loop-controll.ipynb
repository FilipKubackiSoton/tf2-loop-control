{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow2 training loop control using default *tf.fit(...)* function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Descripton\n",
    "\n",
    "Up to now custom training loop in Tensorflow2 requires writing two lops:\n",
    "1. loop iterating through epochs \n",
    "2. loop iterating through batches \n",
    "\n",
    "Then all castom training precudere will have to be implemented in these double-loop block of code. It's neither elegant nor robust due to the missing advanced features of *tf.fit(...)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 10:46:56.111242: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-23 10:46:56.111287: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind(instance, func, as_name=None):\n",
    "    \"\"\"\n",
    "    Bind the function *func* to *instance*, with either provided name *as_name*\n",
    "    or the existing name of *func*. The provided *func* should accept the \n",
    "    instance as the first argument, i.e. \"self\".\n",
    "    \"\"\"\n",
    "    if as_name is None:\n",
    "        as_name = func.__name__\n",
    "    bound_method = func.__get__(instance, instance.__class__)\n",
    "    setattr(instance, as_name, bound_method)\n",
    "    return bound_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-23 10:47:09.376649: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-23 10:47:09.376722: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-23 10:47:09.376787: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (filip-HP-ProBook-440-G3): /proc/driver/nvidia/version does not exist\n",
      "2023-02-23 10:47:09.384485: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "DATASET_SIZE, INPUT_SIZE, OUTPUT_SIZE = 1000, 2, 1\n",
    "BATCH_SIZE = 64\n",
    "data = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform((DATASET_SIZE, INPUT_SIZE)), tf.random.uniform((DATASET_SIZE, OUTPUT_SIZE)))\n",
    "    ).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopControlerCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, config: int, default_in_branch: Dict[str, Any] = None, verbose: bool = True, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super(LoopControlerCallback, self).__init__(*args, **kwargs)\n",
    "        self.default_in_branch: Dict[str, Any] = default_in_branch if default_in_branch else {\n",
    "                \"loss\": None, \n",
    "                \"clipping\": None,\n",
    "                \"variables\": None,\n",
    "                \"excluded_variables\": None,\n",
    "            }\n",
    "        self.config: Dict[str, Any] = config\n",
    "        self.verbose: bool = verbose\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"Function called directely before training. It is executed on each call of model.fit with this callback.\n",
    "            Inside the scope of this funciton we can access model on which this callback works: self.model.\n",
    "\n",
    "        Args:\n",
    "            logs (_type_, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        # meta attributes for building conditions\n",
    "        self.epochs: int = 0\n",
    "        self.batches: int = 0\n",
    "        self.batches_in_epoch: int = 0\n",
    "        self.last_loss: int = 0.0\n",
    "        self.history: List[Any] = []\n",
    "        \n",
    "        # if loss in default branch is None, then use compiled loss\n",
    "        if not self.default_in_branch[\"loss\"]:\n",
    "            self.default_in_branch[\"loss\"] = self.model.compiled_loss\n",
    "\n",
    "        # extend config with validation step\n",
    "        self.config = self._extend_config(self.config)\n",
    "\n",
    "        # assign active and exluded variables for slave steps\n",
    "        self._extract_substeps_varialbe_arrays(self.config)\n",
    "\n",
    "        # bind control variables and control conditions\n",
    "        self._bind_controlers(self.config)\n",
    "        \n",
    "        # bind master train_step to model\n",
    "        self._bind_master_step(self.config)\n",
    "\n",
    "        # bind slave train_steps to model\n",
    "        self._bind_slaves_steps(self.config)\n",
    "        \n",
    "    def _extract_substeps_varialbe_arrays(self, config: Dict[str, Any]) -> None:\n",
    "        # keep varaibles from variable attribute from config file\n",
    "        self.model._included_variables = {}\n",
    "        # keep varaibles from excluded_variable attribute from config file\n",
    "        self.model._excluded_variables = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            self.model._included_variables[action_name]={}\n",
    "            self.model._excluded_variables[action_name]={}\n",
    "\n",
    "            if True in action_config and action_config[True][\"variables\"]:\n",
    "                get_vars = action_config[True][\"variables\"]\n",
    "                bind(self.model, get_vars)\n",
    "                self.model._included_variables[action_name][True] = getattr(self.model, get_vars.__name__)()\n",
    "            if False in action_config and action_config[False][\"variables\"]:\n",
    "                    get_vars = action_config[False][\"variables\"]\n",
    "                    bind(self.model, get_vars)\n",
    "                    self.model._included_variables[action_name][False] = getattr(self.model, get_vars.__name__)()\n",
    "                    \n",
    "            if True in action_config and action_config[True][\"excluded_variables\"]:\n",
    "                get_vars = action_config[True][\"excluded_variables\"]\n",
    "                bind(self.model, get_vars)\n",
    "                self.model._excluded_variables[action_name][True] = getattr(self.model, get_vars.__name__)()            \n",
    "            if False in action_config and action_config[False][\"excluded_variables\"]:\n",
    "                    get_vars = action_config[False][\"excluded_variables\"]\n",
    "                    bind(self.model, get_vars)\n",
    "                    self.model._excluded_variables[action_name][False] = getattr(self.model, get_vars.__name__)()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"-------------------Variables Evaluated for 'variable'/'excluded_varaibles'-------------------\\n\")\n",
    "            print(\"-------------------model._included_variables-------------------\")\n",
    "            print(self.model._included_variables)\n",
    "            print(\"\\n-------------------model._excluded_variables-------------------\")\n",
    "            print(self.model._excluded_variables)\n",
    "\n",
    "    def _bind_master_step(self, config: Dict[str, Any]) -> None:\n",
    "        lscope = locals()\n",
    "        def _get_losses(config: Dict[str, Any]) -> str:\n",
    "            def _substeps_condition_execution(name: str, config: Dict[str, Any], on: bool) -> str:\n",
    "                if on:\n",
    "                    return f\"self.{name}_on(data)\" if True in config else \"0.0\"\n",
    "                else:\n",
    "                    return f\"self.{name}_off(data)\" if False in config else \"0.0\"\n",
    "                    \n",
    "            return \"{\" + \",\".join([\n",
    "                        f\"'loss_{an}' : tf.cond(self.control_variables['{an}'], lambda: {_substeps_condition_execution(an, ac, True)}, lambda: {_substeps_condition_execution(an, ac, False)})\"\n",
    "                        for an, ac in config.items()\n",
    "                    ]) + \"}\"\n",
    "\n",
    "        lscope = locals()\n",
    "        function_body = \"\"\"\n",
    "@tf.function\n",
    "def train_step(self, data):\n",
    "    loss = {losses_config}\n",
    "    metrics = {{m.name : m.result() for m in self.metrics}}\n",
    "    \n",
    "    control_states = {{\n",
    "            control_name: tf.cond(\n",
    "                control_value,\n",
    "                lambda: tf.constant(True),\n",
    "                lambda: tf.constant(False),\n",
    "            )\n",
    "            for control_name, control_value in self.control_variables.items()\n",
    "    }}\n",
    "    \n",
    "    return {{**loss, **metrics, **control_states}}\n",
    "\"\"\".format(\n",
    "            **{\"losses_config\": _get_losses(config)}\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\n-------------------MASTER STEP-------------------\")\n",
    "            print(function_body)\n",
    "        \n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[\"train_step\"])\n",
    "\n",
    "    def _bind_slaves_steps(self, config) -> None:\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"-------------------SLAVE STEPS-------------------\\n\")\n",
    "\n",
    "        for action_name, action_config in config.items():\n",
    "            if True in action_config:\n",
    "                self._bind_slave_step(action_name, action_config[True], True)\n",
    "            if False in action_config:\n",
    "                self._bind_slave_step(action_name, action_config[False], False)\n",
    "\n",
    "    \n",
    "    def _bind_slave_step(self, action_name: str, fn_config: Dict[str, Any], branch: bool) -> None:\n",
    "        \n",
    "        lscope = {\n",
    "            **locals(), \n",
    "            **fn_config\n",
    "            }\n",
    "        fn_name = self._get_actoin_step_name(action_name, branch)\n",
    "\n",
    "        function_body = f\"\"\"\n",
    "@tf.function\n",
    "def {fn_name}(self, data):\n",
    "    x, y = data\n",
    "    with tf.GradientTape(watch_accessed_variables={'False' if fn_config[\"variables\"] or fn_config[\"excluded_variables\"] else 'True'}) as tape:\n",
    "        \"\"\"\n",
    "\n",
    "        if fn_config[\"variables\"] or fn_config[\"excluded_variables\"]:\n",
    "            function_body += f\"\"\"\n",
    "        for g in self.{'_included_variables' if fn_config[\"variables\"] else '_excluded_variables' }['{action_name}'][{branch}]:\n",
    "            tape.watch(g)\n",
    "            \"\"\"\n",
    "\n",
    "        function_body += \"\"\"\n",
    "        logits = self(x, training=True)\n",
    "        loss_value = loss(y, logits)\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip({clipping_grads}, tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "    return loss_value\n",
    "\"\"\".format(**{\n",
    "    \"clipping_grads\": \"[tf.clip_by_value(g, {clip_low}, {clip_high}) for g in grads]\".format(**{\n",
    "        \"clip_low\": fn_config[\"clipping\"][0], \"clip_high\": fn_config[\"clipping\"][1]\n",
    "        }) if fn_config[\"clipping\"] else \"grads\",\n",
    "})\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"-------------------{fn_name}-------------------\")\n",
    "            print(function_body)\n",
    "        \n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[fn_name])\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs) -> None:\n",
    "        self.epochs += 1\n",
    "        \"\"\"Control gating variable from the level of callback which can work on epoch/batch level.\"\"\"\n",
    "        # tf.variable.assign is different than tf.variable = <sth>. The second option is compiled to static\n",
    "        # value in TF graph of computation as the result of @tf.function decorators in LoopControlableModel\n",
    "        for action_name, _ in self.config.items():\n",
    "            self.model.control_variables[action_name].assign(\n",
    "                getattr(self, self.control_conditions[action_name])()\n",
    "            )\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.batches += 1\n",
    "        \"\"\"Control gating variable from the level of callback which can work on epoch/batch level.\"\"\"\n",
    "        # tf.variable.assign is different than tf.variable = <sth>. The second option is compiled to static\n",
    "        # value in TF graph of computation as the result of @tf.function decorators in LoopControlableModel\n",
    "        for action_name, _ in self.config.items():\n",
    "            self.model.control_variables[action_name].assign(\n",
    "                getattr(self, self.control_conditions[action_name])()\n",
    "            )\n",
    "        \n",
    "    def _get_actoin_step_name(self, action_name: str, branch: bool) -> str:\n",
    "        return f\"{action_name}_on\" if branch else f\"{action_name}_off\"\n",
    "\n",
    "    def _bind_controlers(self, config) -> None:\n",
    "        self.model.control_variables = {}\n",
    "        self.control_conditions = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            self.model.control_variables[action_name] = tf.Variable(False, trainable=False)\n",
    "            condition_function_name = action_name + \"_condition\"\n",
    "            bind(self, action_config[\"cond\"], condition_function_name)\n",
    "            self.control_conditions[action_name] = condition_function_name\n",
    "\n",
    "    def _extend_config(self, config: Dict[str, Any]) -> None:\n",
    "        \"\"\"Extend and validate config file. Fill missing fields based on the default_in_branch.\n",
    "\n",
    "        Args:\n",
    "            config (Dict[str, Any]): Configuration to control model training\n",
    "        \"\"\"\n",
    "\n",
    "        def validate_action_config(action_name: str, action_config: Dict[str, Any]) -> None:\n",
    "            \"\"\"Validate model training configuration.\n",
    "\n",
    "            Args:\n",
    "                action_name (str): name of the action slave train step\n",
    "                action_config (Dict[str, Any]): configuration of the action slave train step\n",
    "\n",
    "            Raises:\n",
    "                ValueError: Missing controlable cond\n",
    "                ValueError: Missing branch configuration for true/false after cond\n",
    "            \"\"\"\n",
    "\n",
    "            Warning(f\"------Validating Configuration for {action_name}------\")\n",
    "            if action_config == {}:\n",
    "                Warning(\n",
    "                    f\"{action_name} has empty body. Condition and False or True branch must be implemented.\\n It's ignored in furhter computations\"\n",
    "                )\n",
    "            if (True not in action_config) and (False not in action_config):\n",
    "                raise ValueError(\n",
    "                    f\"{action_name} has no False or True branch implemented\"\n",
    "                )\n",
    "            if \"cond\" not in action_config:\n",
    "                raise ValueError(f\"{action_name} has no condition implemented.\")\n",
    "\n",
    "        pc = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            validate_action_config(action_name, action_config)\n",
    "            pc[action_name] = {\"cond\": action_config[\"cond\"]}\n",
    "            if True in action_config:\n",
    "                pc[action_name][True] = {**self.default_in_branch, **action_config[True]}\n",
    "            if False in action_config:\n",
    "                pc[action_name][False] = {**self.default_in_branch, **action_config[False]}\n",
    "\n",
    "        return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------Variables Evaluated for 'variable'/'excluded_varaibles'-------------------\n",
      "\n",
      "-------------------model._included_variables-------------------\n",
      "{'delay': DictWrapper({}), 'gate': DictWrapper({False: ListWrapper([<tf.Variable 'dense_26/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[0.24976492],\n",
      "       [0.37335157]], dtype=float32)>])})}\n",
      "\n",
      "-------------------model._excluded_variables-------------------\n",
      "{'delay': DictWrapper({}), 'gate': DictWrapper({True: ListWrapper([<tf.Variable 'dense_26/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
      "array([[0.24976492],\n",
      "       [0.37335157]], dtype=float32)>])})}\n",
      "\n",
      "-------------------MASTER STEP-------------------\n",
      "\n",
      "@tf.function\n",
      "def train_step(self, data):\n",
      "    loss = {'loss_delay' : tf.cond(self.control_variables['delay'], lambda: self.delay_on(data), lambda: 0.0),'loss_gate' : tf.cond(self.control_variables['gate'], lambda: self.gate_on(data), lambda: self.gate_off(data))}\n",
      "    metrics = {m.name : m.result() for m in self.metrics}\n",
      "    \n",
      "    control_states = {\n",
      "            control_name: tf.cond(\n",
      "                control_value,\n",
      "                lambda: tf.constant(True),\n",
      "                lambda: tf.constant(False),\n",
      "            )\n",
      "            for control_name, control_value in self.control_variables.items()\n",
      "    }\n",
      "    \n",
      "    return {**loss, **metrics, **control_states}\n",
      "\n",
      "-------------------SLAVE STEPS-------------------\n",
      "\n",
      "-------------------delay_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def delay_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "        \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    \n",
      "\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_on(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      "        \n",
      "        for g in self._excluded_variables['gate'][True]:\n",
      "            tape.watch(g)\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    \n",
      "\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.2, 0.3) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_off-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_off(self, data):\n",
      "    x, y = data\n",
      "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      "        \n",
      "        for g in self._included_variables['gate'][False]:\n",
      "            tape.watch(g)\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    \n",
      "\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 2s 3ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.4800 - mae: 0.4778 - delay: 0.0000e+00 - gate: 0.5294 \n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.4668 - mae: 0.4646 - delay: 0.0000e+00 - gate: 0.5294\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.4558 - mae: 0.4535 - delay: 0.0000e+00 - gate: 0.5294\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.4455 - mae: 0.4434 - delay: 0.0000e+00 - gate: 0.5294\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.4357 - mae: 0.4336 - delay: 0.0000e+00 - gate: 0.5294\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_delay: 0.1311 - loss_gate: 0.2905 - mae: 0.2983 - delay: 1.0000 - gate: 0.5294\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_delay: 0.0998 - loss_gate: 0.2647 - mae: 0.2666 - delay: 1.0000 - gate: 0.5294\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_delay: 0.0959 - loss_gate: 0.2601 - mae: 0.2623 - delay: 1.0000 - gate: 0.5294\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 6ms/step - loss_delay: 0.0930 - loss_gate: 0.2571 - mae: 0.2593 - delay: 1.0000 - gate: 0.5294\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss_delay: 0.0911 - loss_gate: 0.2551 - mae: 0.2574 - delay: 1.0000 - gate: 0.5294\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(INPUT_SIZE),\n",
    "    tf.keras.layers.Dense(OUTPUT_SIZE),\n",
    "    tf.keras.layers.Dense(OUTPUT_SIZE)\n",
    "    ])\n",
    "# compile model\n",
    "model.compile(optimizer=\n",
    "    tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mae\"])\n",
    "\n",
    "def gate_config(self):\n",
    "    return self.batches % 2 == 1\n",
    "\n",
    "def delay_config(self):\n",
    "    return self.epochs > 5\n",
    "\n",
    "def dummy_vars(self):\n",
    "    if 2 < 3:\n",
    "        return [self.variables[0]]\n",
    "    return [self.variables[1]]\n",
    "\n",
    "config = {\n",
    "    \"delay\": {\n",
    "        \"cond\": delay_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanSquaredError(), \n",
    "        },\n",
    "    },\n",
    "    \"gate\": {\n",
    "        \"cond\": gate_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanAbsoluteError(),\n",
    "            \"clipping\": (-0.2, 0.3),\n",
    "            \"excluded_variables\": dummy_vars\n",
    "        },\n",
    "        False: {\n",
    "            \"loss\": tf.keras.losses.MeanAbsoluteError(),\n",
    "            \"variables\": dummy_vars,\n",
    "\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "lcc = LoopControlerCallback(config, verbose=1)\n",
    "# start training\n",
    "history = model.fit(data, epochs = 10, verbose=1,\n",
    "    callbacks=[lcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.utils import types\n",
    "from typeguard import typechecked\n",
    "\n",
    "class NALURegularizer(tf.keras.regularizers.Regularizer):\n",
    "    def __init__(self, reg_coef=0.1):\n",
    "        self.reg_coef = reg_coef\n",
    "\n",
    "    def __call__(self, var: List[tf.Variable]) -> tf.Tensor:\n",
    "        return self.reg_coef * tf.add_n(\n",
    "            [\n",
    "                tf.reduce_mean(tf.math.maximum(tf.math.minimum(-v, v) + 20, 0))\n",
    "                for v in var\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"reg_coef\": float(self.reg_coef)}\n",
    "\n",
    "class NALU(tf.keras.layers.Layer):\n",
    "\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        units: int,\n",
    "        regularizer: types.Regularizer = NALURegularizer(reg_coef=0.05),\n",
    "        clipping: float = 20,\n",
    "        w_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=1.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        m_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=-1.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        g_initializer: types.Initializer = tf.random_normal_initializer(\n",
    "            mean=0.0, stddev=0.1, seed=None\n",
    "        ),\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(NALU, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.units = units\n",
    "        self.reg_fn = regularizer\n",
    "        self.clipping = clipping\n",
    "\n",
    "        self.w_initializer = w_initializer\n",
    "        self.m_initializer = m_initializer\n",
    "        self.g_initializer = g_initializer\n",
    "\n",
    "        self.gate_as_vector = True\n",
    "        self.force_operation = None\n",
    "        self.weights_separation = True\n",
    "        self.input_gate_dependance = False\n",
    "        self.initializer = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # action variables\n",
    "        self.w_hat = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.w_initializer,\n",
    "            trainable = True,\n",
    "            name = \"w\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        self.m_hat = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.m_initializer,\n",
    "            trainable = True,\n",
    "            name = \"m\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        self.w_hat_prime = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.w_initializer,\n",
    "            trainable = True,\n",
    "            name = \"w_prime\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        self.m_hat_prime = self.add_weight (\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = self.m_initializer,\n",
    "            trainable = True,\n",
    "            name = \"m_prime\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "        # gating varaible\n",
    "        self.g = self.add_weight (\n",
    "            shape = (self.units, ),\n",
    "            initializer = self.g_initializer,\n",
    "            trainable = False,\n",
    "            name = \"g\",\n",
    "            use_resource = False\n",
    "        )\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def get_reg_loss(self):\n",
    "        var_list = [self.w_hat, self.m_hat, self.g]\n",
    "        if self.weights_separation:\n",
    "            var_list += [self.w_hat_prime, self.m_hat_prime]\n",
    "        return self.reg_fn(var_list)\n",
    "\n",
    "    def call(self, input):\n",
    "        eps = 1e-7\n",
    "        w1 = tf.math.tanh(self.w_hat) * tf.math.sigmoid(self.m_hat)\n",
    "        w2 = tf.math.tanh(self.w_hat_prime) * tf.math.sigmoid(self.m_hat_prime)\n",
    "        a1 = tf.matmul(input, w1)\n",
    "\n",
    "        m1 = tf.math.exp(tf.minimum(tf.matmul(tf.math.log(tf.maximum(tf.math.abs(input), eps)),w2), self.clipping))\n",
    "        \n",
    "        # sign\n",
    "        w1s = tf.math.abs(tf.reshape(w2, [-1]))\n",
    "        xs = tf.concat([input] * w1.shape[1], axis=1)\n",
    "        xs = tf.reshape(xs, shape=[-1, w1.shape[0] * w1.shape[1]])\n",
    "        sgn = tf.sign(xs) * w1s + (1 - w1s)\n",
    "        sgn = tf.reshape(sgn, shape=[-1, w1.shape[1], w1.shape[0]])\n",
    "        ms = tf.math.reduce_prod(sgn, axis=2)\n",
    "        \n",
    "        self.add_loss(lambda: self.get_reg_loss())\n",
    "\n",
    "        g1 = tf.math.sigmoid(self.g)\n",
    "        return g1 * a1 + (1 - g1) * m1 * tf.clip_by_value(ms, -1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t -3\n",
      "\t\tmean(s)\t 0.0\n",
      "\t\tdata <\t 3\n",
      "\t\tstd \t 1.0\n",
      "Generating Data: \n",
      "Int: \tdist \t normal\n",
      "\t\tdata >=\t 10\n",
      "\t\tmean(s)\t 12.5\n",
      "\t\tdata <\t 15\n",
      "\t\tstd \t 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "from scipy.stats import truncnorm\n",
    "import ast\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-o\", \"--output\", dest=\"output\", default=\"naly_syn_simple_arith\")\n",
    "parser.add_argument(\"-d\", \"--dist\", dest=\"dist\", default=\"normal\", help=\"Prob.Dist\")\n",
    "parser.add_argument(\n",
    "    \"-p\", \"--params\", dest=\"params\", default=\"(-3,3)\", type=ast.literal_eval\n",
    ")\n",
    "parser.add_argument(\"-e\", \"--ext\", dest=\"ext\", default=\"(10,15)\", type=ast.literal_eval)\n",
    "\n",
    "parser.add_argument(\"-n\", \"--nalu\", dest=\"nalu\", default=\"nalui1\")\n",
    "parser.add_argument(\"-se\", \"--seed\", dest=\"seed\", default=42, type=int)\n",
    "parser.add_argument(\"-op\", \"--operation\", dest=\"op\", default=\"MUL\")\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "def sample(dist, params, numDim=3, numDP=64000):\n",
    "    data = np.zeros(shape=(numDP, numDim))\n",
    "    if dist == \"normal\":\n",
    "        intmean = (params[0] + params[1]) / 2\n",
    "        intstd = (params[1] - params[0]) / 6\n",
    "        print(\n",
    "            \"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tmean(s)\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\tstd \\t %s\"\n",
    "            % (dist, params[0], intmean, params[1], intstd)\n",
    "        )\n",
    "        mi, ma = (params[0] - intmean) / intstd, (params[1] - intmean) / intstd\n",
    "        data = np.reshape(\n",
    "            truncnorm.rvs(mi, ma, intmean, intstd, size=numDim * numDP), data.shape\n",
    "        )\n",
    "\n",
    "    elif dist == \"uniform\":\n",
    "        print(\n",
    "            \"Generating Data: \\nInt: \\tdist \\t %s\\n\\t\\tdata >=\\t %s\\n\\t\\tdata <\\t %s\\n\\t\\t\"\n",
    "            % (dist, params[0], params[1])\n",
    "        )\n",
    "        data = np.reshape(\n",
    "            np.random.uniform(params[0], params[1], size=numDim * numDP), data.shape\n",
    "        )\n",
    "    elif dist == \"exponential\":\n",
    "        data = np.random.exponential(params, size=(numDP, numDim))\n",
    "    else:\n",
    "        raise Exception(\"Unknown distribution\")\n",
    "    data = np.reshape(data, [-1])  # reshape to mix both distributions per instance!\n",
    "    np.random.shuffle(data)\n",
    "    data = np.reshape(data, (numDP, numDim))\n",
    "    return data\n",
    "\n",
    "\n",
    "def operation(op, a, b):\n",
    "    if op.lower() == \"mul\":\n",
    "        return a * b\n",
    "    if op.lower() == \"add\":\n",
    "        return a + b\n",
    "    if op.lower() == \"sub\":\n",
    "        return a - b\n",
    "    if op.lower() == \"div\":\n",
    "        return a / b\n",
    "\n",
    "input_dim = 7\n",
    "output_dim = 1\n",
    "\n",
    "def data_comb(data):\n",
    "    return (data[:, 0] - data[:, 1]) *  (data[:, 2] - data[:, 3]) +  (data[:, 4] * data[:, 5]) \n",
    "\n",
    "data = sample(args.dist, args.params, input_dim)\n",
    "lbls =  data_comb(data)\n",
    "lbls = np.reshape(lbls, newshape=(-1, 1))\n",
    "\n",
    "int_data = sample(args.dist, args.params, input_dim)\n",
    "int_lbls =  data_comb(int_data)\n",
    "int_lbls = np.reshape(int_lbls, newshape=(-1, 1))\n",
    "\n",
    "\n",
    "ext_data = sample(args.dist, args.ext, input_dim)\n",
    "ext_lbls =  data_comb(ext_data)\n",
    "ext_lbls = np.reshape(ext_lbls, newshape=(-1, 1))\n",
    "\n",
    "\n",
    "data_dp = tf.data.Dataset.from_tensor_slices((data, lbls)).prefetch(tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "int_data_dp = tf.data.Dataset.from_tensor_slices((int_data, int_lbls)).prefetch(tf.data.AUTOTUNE)\n",
    "ext_data_dp = tf.data.Dataset.from_tensor_slices((ext_data, ext_lbls)).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------MASTER STEP-------------------\n",
      "\n",
      "@tf.function\n",
      "def train_step(self, data):\n",
      "    loss = {'loss_delay' : tf.cond(self.control_variables['delay'], lambda: self.delay_on(data), lambda: 0.0),'loss_gate' : tf.cond(self.control_variables['gate'], lambda: self.gate_on(data), lambda: self.gate_off(data))}\n",
      "    metrics = {m.name : m.result() for m in self.metrics}\n",
      "    \n",
      "    control_states = {\n",
      "            control_name: tf.cond(\n",
      "                control_value,\n",
      "                lambda: tf.constant(True),\n",
      "                lambda: tf.constant(False),\n",
      "            )\n",
      "            for control_name, control_value in self.control_variables.items()\n",
      "    }\n",
      "    \n",
      "    return {**loss, **metrics, **control_states}\n",
      "\n",
      "-------------------SLAVE STEPS-------------------\n",
      "\n",
      "-------------------delay_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def delay_on(self, data):\n",
      "    x, y = data\n",
      "        \n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_on(self, data):\n",
      "    x, y = data\n",
      "        \n",
      "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      "        for g in self._get_gate_on_variables():\n",
      "            tape.watch(g)\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.1, 0.1) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_off-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_off(self, data):\n",
      "    x, y = data\n",
      "        \n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 9s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 1.4259 - mae: 1.4262 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 1.1563 - mae: 1.1568 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.8833 - mae: 0.8836 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.7191 - mae: 0.7194 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.3520 - mae: 0.3522 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.1779 - mae: 0.1779 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.0871 - mae: 0.0871 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.0405 - mae: 0.0405 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.0184 - mae: 0.0184 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.0083 - mae: 0.0083 - delay: 0.0000e+00 - gate: 0.1009\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss_delay: 3.8705e-05 - loss_gate: 0.0037 - mae: 0.0037 - delay: 1.0000 - gate: 0.1009\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss_delay: 7.8943e-06 - loss_gate: 0.0017 - mae: 0.0017 - delay: 1.0000 - gate: 0.1009\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss_delay: 1.5943e-06 - loss_gate: 7.4502e-04 - mae: 7.4549e-04 - delay: 1.0000 - gate: 0.1009\n",
      "Epoch 14/15\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss_delay: 3.2209e-07 - loss_gate: 3.3421e-04 - mae: 3.3439e-04 - delay: 1.0000 - gate: 0.1009\n",
      "Epoch 15/15\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss_delay: 6.4883e-08 - loss_gate: 1.4969e-04 - mae: 1.4976e-04 - delay: 1.0000 - gate: 0.1009\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_dim),\n",
    "    NALU(3, clipping = 20),\n",
    "    NALU(2, clipping = 20),\n",
    "    NALU(1, clipping = 20)\n",
    "    ])\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer=\n",
    "    tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mae\"])\n",
    "\n",
    "def gate_config(self):\n",
    "    return self.batches % 10 > 8\n",
    "\n",
    "def delay_config(self):\n",
    "    return self.epochs > 10\n",
    "\n",
    "def dummy_vars(self):\n",
    "    return [self.variables[0]]\n",
    "    \n",
    "def get_gates_variables(self) -> List[tf.Variable]:\n",
    "    return [l.g for l in self.layers if isinstance(l, NALU)]\n",
    "    \n",
    "config = {\n",
    "    \"delay\": {\n",
    "        \"cond\": delay_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanSquaredError(), \n",
    "        },\n",
    "    },\n",
    "    \"gate\": {\n",
    "        \"cond\": gate_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanAbsoluteError(),\n",
    "            \"clipping\": (-0.1, 0.1),\n",
    "            \"variables\": get_gates_variables,\n",
    "            \"excluded_variables\": get_gates_variables\n",
    "        },\n",
    "        False: {\n",
    "            \"loss\": tf.keras.losses.MeanAbsoluteError()\n",
    "        }\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "lcc = LoopControlerCallback(config, verbose=1)\n",
    "# start training\n",
    "history = model.fit(data_dp, epochs = 15, verbose = 1,\n",
    "    callbacks=[lcc])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcc.batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b4ff1917ad7fe489d902c3041fcac2959f3e2431a3acc02677f15f306757aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
