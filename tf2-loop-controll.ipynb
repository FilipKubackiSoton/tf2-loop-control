{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow2 training loop control using default *tf.fit(...)* function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Descripton\n",
    "\n",
    "Up to now custom training loop in Tensorflow2 requires writing two lops:\n",
    "1. loop iterating through epochs \n",
    "2. loop iterating through batches \n",
    "\n",
    "Then all castom training precudere will have to be implemented in these double-loop block of code. It's neither elegant nor robust due to the missing advanced features of *tf.fit(...)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-20 08:16:13.607055: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-20 08:16:13.607085: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bind(instance, func, as_name=None):\n",
    "    \"\"\"\n",
    "    Bind the function *func* to *instance*, with either provided name *as_name*\n",
    "    or the existing name of *func*. The provided *func* should accept the \n",
    "    instance as the first argument, i.e. \"self\".\n",
    "    \"\"\"\n",
    "    if as_name is None:\n",
    "        as_name = func.__name__\n",
    "    bound_method = func.__get__(instance, instance.__class__)\n",
    "    setattr(instance, as_name, bound_method)\n",
    "    return bound_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE, INPUT_SIZE, OUTPUT_SIZE = 1000, 2, 1\n",
    "BATCH_SIZE = 64\n",
    "data = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform((DATASET_SIZE, INPUT_SIZE)), tf.random.uniform((DATASET_SIZE, OUTPUT_SIZE)))\n",
    "    ).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopControlerCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, config: int, default_in_branch: Dict[str, Any] = None, verbose: bool = True, *args, **kwargs\n",
    "    ) -> None:\n",
    "        super(LoopControlerCallback, self).__init__(*args, **kwargs)\n",
    "        self.default_in_branch: Dict[str, Any] = default_in_branch if default_in_branch else {\n",
    "                \"loss\": None, \n",
    "                \"clipping\": None,\n",
    "                \"variables\": None,\n",
    "                \"exclud_var\": None,\n",
    "            }\n",
    "        self.config: Dict[str, Any] = config\n",
    "        self.verbose: bool = verbose\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        \"\"\"Function called directely before training. It is executed on each call of model.fit with this callback.\n",
    "            Inside the scope of this funciton we can access model on which this callback works: self.model.\n",
    "\n",
    "        Args:\n",
    "            logs (_type_, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        # meta attributes for building conditions\n",
    "        self.epochs: int = 0\n",
    "        self.batches: int = 0\n",
    "        self.batches_in_epoch: int = 0\n",
    "        self.last_loss: int = 0.0\n",
    "        self.history: List[Any] = []\n",
    "        \n",
    "        # if loss in default branch is None, then use compiled loss\n",
    "        if not self.default_in_branch[\"loss\"]:\n",
    "            self.default_in_branch[\"loss\"] = self.model.compiled_loss\n",
    "\n",
    "        # extend config with validation step\n",
    "        self.config = self._extend_config(self.config)\n",
    "\n",
    "        # bind control variables and control conditions\n",
    "        self._bind_controlers(self.config)\n",
    "        \n",
    "        # bind master train_step to model\n",
    "        self._bind_master_step(self.config)\n",
    "\n",
    "        # bind slave train_steps to model\n",
    "        self._bind_slaves_steps(self.config)\n",
    "        \n",
    "\n",
    "    def _bind_master_step(self, config: Dict[str, Any]) -> None:\n",
    "        lscope = locals()\n",
    "        def _get_losses(config: Dict[str, Any]) -> str:\n",
    "            def _substeps_condition_execution(name: str, config: Dict[str, Any], on: bool) -> str:\n",
    "                if on:\n",
    "                    return f\"self.{name}_on(data)\" if True in config else \"0.0\"\n",
    "                else:\n",
    "                    return f\"self.{name}_off(data)\" if False in config else \"0.0\"\n",
    "                    \n",
    "            return \"{\" + \",\".join([\n",
    "                        f\"'loss_{an}' : tf.cond(self.control_variables['{an}'], lambda: {_substeps_condition_execution(an, ac, True)}, lambda: {_substeps_condition_execution(an, ac, False)})\"\n",
    "                        for an, ac in config.items()\n",
    "                    ]) + \"}\"\n",
    "\n",
    "        lscope = locals()\n",
    "        function_body = \"\"\"\n",
    "@tf.function\n",
    "def train_step(self, data):\n",
    "    loss = {losses_config}\n",
    "    metrics = {{m.name : m.result() for m in self.metrics}}\n",
    "    \n",
    "    control_states = {{\n",
    "            control_name: tf.cond(\n",
    "                control_value,\n",
    "                lambda: tf.constant(True),\n",
    "                lambda: tf.constant(False),\n",
    "            )\n",
    "            for control_name, control_value in self.control_variables.items()\n",
    "    }}\n",
    "    \n",
    "    return {{**loss, **metrics, **control_states}}\n",
    "\"\"\".format(\n",
    "            **{\"losses_config\": _get_losses(config)}\n",
    "        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"-------------------MASTER STEP-------------------\")\n",
    "            print(function_body)\n",
    "        \n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[\"train_step\"])\n",
    "\n",
    "    def _bind_slaves_steps(self, config) -> None:\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"-------------------SLAVE STEPS-------------------\\n\")\n",
    "\n",
    "        for action_name, action_config in config.items():\n",
    "            if True in action_config:\n",
    "                self._bind_slave_step(action_name, action_config[True], True)\n",
    "            if False in action_config:\n",
    "                self._bind_slave_step(action_name, action_config[False], False)\n",
    "\n",
    "    def _bind_slave_step(self, action_name: str, fn_config: Dict[str, Any], branch: bool) -> None:\n",
    "        \n",
    "        lscope = {\n",
    "            **locals(), \n",
    "            **fn_config\n",
    "            }\n",
    "        fn_name = self._get_actoin_step_name(action_name, branch)\n",
    "\n",
    "        function_body = f\"\"\"\n",
    "@tf.function\n",
    "def {fn_name}(self, data):\n",
    "    x, y = data\n",
    "        \"\"\"\n",
    "\n",
    "        if fn_config[\"variables\"]:\n",
    "            get_vars_fn = \"_get_\" + fn_name + \"_variables\"\n",
    "            bind(self.model, fn_config[\"variables\"], get_vars_fn)\n",
    "            function_body += f\"\"\"\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "        for g in self.{get_vars_fn}():\n",
    "            tape.watch(g)\n",
    "            \"\"\"\n",
    "        else:\n",
    "            function_body += \"\"\"\n",
    "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
    "            \"\"\"\n",
    "\n",
    "        function_body += \"\"\"\n",
    "        logits = self(x, training=True)\n",
    "        loss_value = loss(y, logits)\n",
    "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
    "    self.optimizer.apply_gradients(zip({clipping_grads}, tape.watched_variables()))\n",
    "    self.compiled_metrics.update_state(y, logits)\n",
    "    return loss_value\n",
    "\"\"\".format(**{\n",
    "    \"clipping_grads\": \"[tf.clip_by_value(g, {clip_low}, {clip_high}) for g in grads]\".format(**{\n",
    "        \"clip_low\": fn_config[\"clipping\"][0], \"clip_high\": fn_config[\"clipping\"][1]\n",
    "        }) if fn_config[\"clipping\"] else \"grads\",\n",
    "})\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"-------------------{fn_name}-------------------\")\n",
    "            print(function_body)\n",
    "        \n",
    "        exec(function_body, {**globals(), **lscope}, lscope)\n",
    "        bind(self.model, lscope[fn_name])\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs) -> None:\n",
    "        self.epochs += 1\n",
    "        \"\"\"Control gating variable from the level of callback which can work on epoch/batch level.\"\"\"\n",
    "        # tf.variable.assign is different than tf.variable = <sth>. The second option is compiled to static\n",
    "        # value in TF graph of computation as the result of @tf.function decorators in LoopControlableModel\n",
    "        for action_name, _ in self.config.items():\n",
    "            self.model.control_variables[action_name].assign(\n",
    "                getattr(self, self.control_conditions[action_name])()\n",
    "            )\n",
    "\n",
    "    def _get_actoin_step_name(self, action_name: str, branch: bool) -> str:\n",
    "        return f\"{action_name}_on\" if branch else f\"{action_name}_off\"\n",
    "\n",
    "    def _bind_controlers(self, config) -> None:\n",
    "        self.model.control_variables = {}\n",
    "        self.control_conditions = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            self.model.control_variables[action_name] = tf.Variable(False, trainable=False)\n",
    "            condition_function_name = action_name + \"_condition\"\n",
    "            bind(self, action_config[\"cond\"], condition_function_name)\n",
    "            self.control_conditions[action_name] = condition_function_name\n",
    "\n",
    "    def _extend_config(self, config: Dict[str, Any]) -> None:\n",
    "        \"\"\"Extend and validate config file. Fill missing fields based on the default_in_branch.\n",
    "\n",
    "        Args:\n",
    "            config (Dict[str, Any]): Configuration to control model training\n",
    "        \"\"\"\n",
    "\n",
    "        def validate_action_config(action_name: str, action_config: Dict[str, Any]) -> None:\n",
    "            \"\"\"Validate model training configuration.\n",
    "\n",
    "            Args:\n",
    "                action_name (str): name of the action slave train step\n",
    "                action_config (Dict[str, Any]): configuration of the action slave train step\n",
    "\n",
    "            Raises:\n",
    "                ValueError: Missing controlable cond\n",
    "                ValueError: Missing branch configuration for true/false after cond\n",
    "            \"\"\"\n",
    "\n",
    "            Warning(f\"------Validating Configuration for {action_name}------\")\n",
    "            if action_config == {}:\n",
    "                Warning(\n",
    "                    f\"{action_name} has empty body. Condition and False or True branch must be implemented.\\n It's ignored in furhter computations\"\n",
    "                )\n",
    "            if (True not in action_config) and (False not in action_config):\n",
    "                raise ValueError(\n",
    "                    f\"{action_name} has no False or True branch implemented\"\n",
    "                )\n",
    "            if \"cond\" not in action_config:\n",
    "                raise ValueError(f\"{action_name} has no condition implemented.\")\n",
    "\n",
    "        pc = {}\n",
    "        for action_name, action_config in config.items():\n",
    "            validate_action_config(action_name, action_config)\n",
    "            pc[action_name] = {\"cond\": action_config[\"cond\"]}\n",
    "            if True in action_config:\n",
    "                pc[action_name][True] = {**self.default_in_branch, **action_config[True]}\n",
    "            if False in action_config:\n",
    "                pc[action_name][False] = {**self.default_in_branch, **action_config[False]}\n",
    "\n",
    "        return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------MASTER STEP-------------------\n",
      "\n",
      "@tf.function\n",
      "def train_step(self, data):\n",
      "    loss = {'loss_delay' : tf.cond(self.control_variables['delay'], lambda: self.delay_on(data), lambda: 0.0),'loss_gate' : tf.cond(self.control_variables['gate'], lambda: self.gate_on(data), lambda: self.gate_off(data))}\n",
      "    metrics = {m.name : m.result() for m in self.metrics}\n",
      "    \n",
      "    control_states = {\n",
      "            control_name: tf.cond(\n",
      "                control_value,\n",
      "                lambda: tf.constant(True),\n",
      "                lambda: tf.constant(False),\n",
      "            )\n",
      "            for control_name, control_value in self.control_variables.items()\n",
      "    }\n",
      "    \n",
      "    return {**loss, **metrics, **control_states}\n",
      "\n",
      "-------------------SLAVE STEPS-------------------\n",
      "\n",
      "-------------------delay_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def delay_on(self, data):\n",
      "    x, y = data\n",
      "        \n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_on-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_on(self, data):\n",
      "    x, y = data\n",
      "        \n",
      "    with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
      "        for g in self._get_gate_on_variables():\n",
      "            tape.watch(g)\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip([tf.clip_by_value(g, -0.2, 0.3) for g in grads], tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "-------------------gate_off-------------------\n",
      "\n",
      "@tf.function\n",
      "def gate_off(self, data):\n",
      "    x, y = data\n",
      "        \n",
      "    with tf.GradientTape(watch_accessed_variables=True) as tape:\n",
      "            \n",
      "        logits = self(x, training=True)\n",
      "        loss_value = loss(y, logits)\n",
      "    grads = tape.gradient(loss_value, tape.watched_variables())\n",
      "    self.optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
      "    self.compiled_metrics.update_state(y, logits)\n",
      "    return loss_value\n",
      "\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - 2s 3ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.8835 - mae: 0.8952 - delay: 0.0000e+00 - gate: 1.0000\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.4228 - mae: 0.4312 - delay: 0.0000e+00 - gate: 0.0000e+00\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.3041 - mae: 0.3036 - delay: 0.0000e+00 - gate: 1.0000\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.2867 - mae: 0.2859 - delay: 0.0000e+00 - gate: 0.0000e+00\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 0s 3ms/step - loss_delay: 0.0000e+00 - loss_gate: 0.2738 - mae: 0.2727 - delay: 0.0000e+00 - gate: 1.0000\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss_delay: 0.0946 - loss_gate: 0.2625 - mae: 0.2636 - delay: 1.0000 - gate: 0.0000e+00\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss_delay: 0.0898 - loss_gate: 0.2576 - mae: 0.2585 - delay: 1.0000 - gate: 1.0000\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 0s 5ms/step - loss_delay: 0.0895 - loss_gate: 0.2569 - mae: 0.2585 - delay: 1.0000 - gate: 0.0000e+00\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_delay: 0.0891 - loss_gate: 0.2566 - mae: 0.2576 - delay: 1.0000 - gate: 1.0000\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 0s 4ms/step - loss_delay: 0.0895 - loss_gate: 0.2569 - mae: 0.2585 - delay: 1.0000 - gate: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(INPUT_SIZE),\n",
    "    tf.keras.layers.Dense(OUTPUT_SIZE),\n",
    "    tf.keras.layers.Dense(OUTPUT_SIZE)\n",
    "    ])\n",
    "# compile model\n",
    "model.compile(optimizer=\n",
    "    tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    loss=\"mse\", \n",
    "    metrics=[\"mae\"])\n",
    "\n",
    "def gate_config(self):\n",
    "    return self.epochs % 2 == 1\n",
    "\n",
    "def delay_config(self):\n",
    "    return self.epochs > 5\n",
    "\n",
    "def dummy_vars(self):\n",
    "    return [self.variables[0]]\n",
    "\n",
    "config = {\n",
    "    \"delay\": {\n",
    "        \"cond\": delay_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanSquaredError(), \n",
    "        },\n",
    "    },\n",
    "    \"gate\": {\n",
    "        \"cond\": gate_config,\n",
    "        True: {\n",
    "            \"loss\": tf.keras.losses.MeanAbsoluteError(),\n",
    "            \"clipping\": (-0.2, 0.3),\n",
    "            \"variables\": dummy_vars\n",
    "        },\n",
    "        False: {\n",
    "            \"loss\": tf.keras.losses.MeanAbsoluteError()\n",
    "        }\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "lcc = LoopControlerCallback(config, verbose=1)\n",
    "# start training\n",
    "history = model.fit(data, epochs = 10, verbose = 1,\n",
    "    callbacks=[lcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b4ff1917ad7fe489d902c3041fcac2959f3e2431a3acc02677f15f306757aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
